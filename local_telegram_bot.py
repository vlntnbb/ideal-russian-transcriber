from __future__ import annotations

import asyncio
import logging
import os
import tempfile
import time
import json
import threading
import io
import datetime
import sys
import warnings
import uuid
import html as _html
import re
import secrets
import smtplib
import ssl
import subprocess
import threading
import multiprocessing
from email.message import EmailMessage
from typing import Optional

from dotenv import load_dotenv
from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update
from telegram.constants import ChatAction, ChatType, ParseMode
from telegram.error import BadRequest, RetryAfter
from telegram.ext import Application, CallbackQueryHandler, CommandHandler, ContextTypes, MessageHandler, filters
import httpx

from snapscript.core.audio_processor import (
    ASRSegment,
    AudioProcessor,
    GigaAMTranscriptionService,
    TranscriptionCancelled,
    TranscriptionService,
    gigaam_transcribe_worker,
)
from snapscript.utils.logging_utils import setup_logging

# Prevent multiple polling instances in the same repo (avoids Telegram getUpdates Conflict).
def _acquire_bot_lock(lock_path: str) -> None:
    import fcntl

    os.makedirs(os.path.dirname(lock_path) or ".", exist_ok=True)
    lock_file = open(lock_path, "a+", encoding="utf-8")
    try:
        fcntl.flock(lock_file.fileno(), fcntl.LOCK_EX | fcntl.LOCK_NB)
    except OSError as exc:
        raise SystemExit(
            f"Another bot instance seems to be running (lock: {lock_path}). Stop it before starting a new one."
        ) from exc
    # Keep FD alive for the whole process.
    globals()["_BOT_LOCK_FD"] = lock_file


TELEGRAM_TEXT_LIMIT = 4096
STATS_PATH = os.path.join(os.path.dirname(__file__), "asr_stats.json")
GEMINI_API_URL = "https://generativelanguage.googleapis.com/v1beta"
DEFAULT_USAGE_LOG_PATH = os.path.join(os.path.dirname(__file__), "usage_sessions.jsonl")
AUTH_STATE_PATH = os.path.join(os.path.dirname(__file__), "auth_state.json")
DEFAULT_GEMINI_SYSTEM_PROMPT_PATH = os.path.join(os.path.dirname(__file__), "gemini_system_prompt.md")

# Email auth: only this domain can authorize users for Gemini access.
DEFAULT_AUTH_DOMAIN = "bbooster.io"
DEFAULT_AUTH_CODE_TTL_SEC = 15 * 60  # 15 minutes
DEFAULT_AUTH_CODE_MIN_WORDS = 6
DEFAULT_AUTH_CODE_MAX_WORDS = 10

# Open-source fallback model (Ollama).
DEFAULT_OLLAMA_BASE_URL = "http://127.0.0.1:11434"
DEFAULT_OLLAMA_MODEL = "deepseek-r1:8b"
DEFAULT_AUDIO_NORMALIZE_KEY = "norm,–Ω–æ—Ä–º"
DEFAULT_AUDIO_NORMALIZE_FILTER = "adeclick,dynaudnorm=f=500:g=11:p=0.95:m=20"

_USAGE_LOGGER: Optional[logging.Logger] = None

DEFAULT_GEMINI_SYSTEM_PROMPT = """\
–≤–æ—Ç –¥–≤–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –æ–¥–Ω–æ–≥–æ –∞—É–¥–∏–æ –¥–≤—É–º—è —Ä–∞–∑–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –æ–Ω–∏ –∫–æ—Å—è—á–∞—Ç –≤ —Ä–∞–∑–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö. —Ç–≤–æ—è –∑–∞–¥–∞—á–∞ —Å–¥–µ–ª–∞—Ç—å –∏—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç, –≤–∑—è–≤ –∏–∑ –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏ –∏—Å–ø—Ä–∞–≤–∏–≤ —è–≤–Ω—ã–µ –æ—à–∏–±–∫–∏ –∏ —É–±—Ä–∞–≤ –ø–æ–≤—Ç–æ—Ä—ã. –∫–æ–≥–¥–∞ –∫–∞–∫–∏–µ-—Ç–æ —Å–ª–æ–≤–∞ –±—É–¥—É—Ç –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –≤—ã–±–∏—Ä–∞–π —Ç–æ—Ç –≤–∞—Ä–∏–∞–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤—ã–≥–ª—è–¥–∏—Ç –±–æ–ª–µ–µ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º. –µ—Å–ª–∏ –≤ –æ–±–æ–∏—Ö —Å–ª—É—á–∞—è—Ö —Ç–µ–∫—Å—Ç –≤—ã–≥–ª—è–¥–∏—Ç –∫–∞–∫ —Å –æ—à–∏–±–∫–∞–º–∏ - –∏—Å–ø—Ä–∞–≤—å –∏—Å—Ö–æ–¥—è –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –Ω–∏ –≤ –∫–æ–µ–º —Å–ª—É—á–∞–µ –Ω–µ –≤—ã–∫–∏–¥—ã–≤–∞–π –Ω–∏–∫–∞–∫–∏–µ —Å–º—ã—Å–ª–æ–≤—ã–µ —Ñ—Ä–∞–∑—ã –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, —Ç–µ–±–µ –Ω—É–∂–Ω–æ —Å–æ–±–ª—é—Å—Ç–∏ —Ç–æ—á–Ω–æ—Å—Ç—å –ø–µ—Ä–µ–¥–∞—á–∏ —Ç–µ–∫—Å—Ç–∞. –ø–æ–¥—Å–≤–µ—Ç–∏ –∂–∏—Ä–Ω—ã–º —Ç–µ —á–∞—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞, –≥–¥–µ —Ç–µ–±–µ –ø—Ä–∏—à–ª–æ—Å—å –∏—Å–ø—Ä–∞–≤–ª—è—Ç—å —Ç–µ–∫—Å—Ç (–≥–¥–µ –æ–Ω –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –º–µ–∂–¥—É –º–æ–¥–µ–ª—è–º–∏).

–æ–±—Ä–∞—â–∞–π –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —É–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä–∞, —á—Ç–æ –Ω–∞–¥–æ –Ω–∞–π—Ç–∏, –∏—Å–ø—Ä–∞–≤–∏—Ç—å –∏ —É—Ç–æ—á–Ω–∏—Ç—å –ø–æ —ç—Ç–æ–º—É —Ç–µ–∫—Å—Ç—É –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å. –≤ –∫–æ–Ω—Ü–µ –æ—Ç—á–∏—Ç–∞–π—Å—è –∫–∞–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è —Å —Ç–µ–∫—Å—Ç–∞–º–∏ —Ç—ã —Å–¥–µ–ª–∞–ª

–≤–æ—Ç —Ç–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç –≤—ã–¥–∞—á–∏ (—à–∞–±–ª–æ–Ω)

### –ò—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç:

### –ü—Ä–∏–º–µ—á–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä–∞:

### –û—Ç—á–µ—Ç –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö:
"""


def _read_text_file(path: str) -> str:
    with open(path, "r", encoding="utf-8") as f:
        return f.read()


def _get_system_prompt() -> tuple[str, str]:
    """
    Returns (prompt, source) where source is one of: file/env/default.
    Priority: GEMINI_SYSTEM_PROMPT_FILE -> GEMINI_SYSTEM_PROMPT -> default constant.
    """
    p = (os.environ.get("GEMINI_SYSTEM_PROMPT_FILE") or "").strip()
    candidate = p or DEFAULT_GEMINI_SYSTEM_PROMPT_PATH
    try:
        if candidate and os.path.exists(candidate):
            out = (_read_text_file(candidate) or "").strip()
            if out:
                return out, f"file:{candidate}"
    except Exception:
        logging.getLogger("local_telegram_bot").exception("Failed to read system prompt file: %s", candidate)

    env_prompt = (os.environ.get("GEMINI_SYSTEM_PROMPT") or "").strip()
    if env_prompt:
        return env_prompt, "env"
    return DEFAULT_GEMINI_SYSTEM_PROMPT.strip(), "default"


def _audio_normalize_key_tokens() -> list[str]:
    raw = (os.environ.get("AUDIO_NORMALIZE_KEY") or "").strip()
    if not raw:
        raw = DEFAULT_AUDIO_NORMALIZE_KEY
    # Allow comma-separated keys.
    keys = [t.strip().lower() for t in raw.split(",") if t.strip()]
    # Convenience aliases: accept both "norm" and "–Ω–æ—Ä–º".
    if "norm" in keys and "–Ω–æ—Ä–º" not in keys:
        keys.append("–Ω–æ—Ä–º")
    if "–Ω–æ—Ä–º" in keys and "norm" not in keys:
        keys.append("norm")
    return keys


def _audio_normalize_filter() -> str:
    return (os.environ.get("AUDIO_NORMALIZE_FILTER") or DEFAULT_AUDIO_NORMALIZE_FILTER).strip()


def _should_normalize_audio(text: str, *, bot_username: Optional[str]) -> bool:
    if not text or not bot_username:
        return False
    keys = _audio_normalize_key_tokens()
    if not keys:
        return False

    # Find the bot mention and inspect tokens after it.
    pattern = re.compile(rf"@{re.escape(bot_username)}\b(.*)$", re.IGNORECASE | re.DOTALL)
    m = pattern.search(text)
    if not m:
        return False
    tail = (m.group(1) or "").strip()
    if not tail:
        return False
    for tok in re.split(r"\s+", tail):
        tok = (tok or "").strip()
        if not tok:
            continue
        tok = tok.strip(".,;:!?()[]{}<>\"'")
        tok = tok.split("=", 1)[0].split(":", 1)[0].strip().lower()
        if tok in keys:
            return True
    return False


def _fmt_dur(sec: float) -> str:
    sec = max(0, int(sec))
    if sec < 60:
        return f"{sec}s"
    m, s = divmod(sec, 60)
    if m < 60:
        return f"{m}m{s:02d}s"
    h, m = divmod(m, 60)
    return f"{h}h{m:02d}m"


def _truthy_env(name: str, default: bool = True) -> bool:
    raw = (os.environ.get(name) or "").strip().lower()
    if not raw:
        return default
    return raw not in {"0", "false", "no", "off", "disabled"}


def _int_env(name: str, default: int) -> int:
    raw = (os.environ.get(name) or "").strip()
    try:
        return int(raw) if raw else int(default)
    except Exception:
        return int(default)


def _bot_concurrent_updates() -> int:
    return max(1, _int_env("BOT_CONCURRENT_UPDATES", 8))


def _asr_concurrency() -> int:
    return max(1, _int_env("ASR_CONCURRENCY", 2))


def _llm_concurrency() -> int:
    return max(1, _int_env("LLM_CONCURRENCY", 2))


_SPELL_WORDS = [
    # Harry Potter-ish (latin-ish) "spell" words, good for manual typing.
    "accio",
    "alohomora",
    "avada",
    "kedavra",
    "crucio",
    "imperio",
    "expelliarmus",
    "stupefy",
    "protego",
    "lumos",
    "nox",
    "expecto",
    "patronum",
    "reducto",
    "sectumsempra",
    "obliviate",
    "wingardium",
    "leviosa",
    "riddikulus",
    "muffliato",
    "finite",
    "incantatem",
    "petrificus",
    "totalus",
    "confundo",
    "diffindo",
    "aguamenti",
    "avis",
    "duro",
    "glacius",
    "silencio",
    "sonorus",
    "quietus",
    "locomotor",
    "morsmordre",
    "reparo",
    "scourgify",
    "tarantallegra",
    "episkey",
    "priori",
    "incarcerous",
    "liberacorpus",
    "legilimens",
    "occlumens",
    "verdimillious",
    "periculum",
    "portus",
    "dissendium",
    "serpensortia",
]

_AUTH_LOCK = threading.Lock()
_AUTH_STATE: Optional[dict] = None


def _auth_enabled() -> bool:
    return _truthy_env("AUTH_ENABLED", False)


def _auth_domain() -> str:
    return (os.environ.get("AUTH_DOMAIN") or DEFAULT_AUTH_DOMAIN).strip().lower()


def _auth_code_ttl_sec() -> int:
    raw = (os.environ.get("AUTH_CODE_TTL_SEC") or "").strip()
    if raw.isdigit():
        return max(60, int(raw))
    raw_min = (os.environ.get("AUTH_CODE_TTL_MIN") or "").strip()
    if raw_min.isdigit():
        return max(1, int(raw_min)) * 60
    return DEFAULT_AUTH_CODE_TTL_SEC


def _auth_code_word_count() -> tuple[int, int]:
    def _int(name: str, default: int) -> int:
        raw = (os.environ.get(name) or "").strip()
        return int(raw) if raw.isdigit() else default

    mn = _int("AUTH_CODE_MIN_WORDS", DEFAULT_AUTH_CODE_MIN_WORDS)
    mx = _int("AUTH_CODE_MAX_WORDS", DEFAULT_AUTH_CODE_MAX_WORDS)
    mn = max(3, mn)
    mx = max(mn, mx)
    return mn, mx


def _auth_state_default() -> dict:
    return {
        "authorized_users": {},  # user_id(str) -> {"email": str, "authorized_at": iso}
        "whitelisted_chats": {},  # chat_id(str) -> {"added_by": user_id, "added_at": iso}
        "pending": {},  # user_id(str) -> {"email": str, "code": str, "expires_at": iso, "attempts": int}
        "prompted_users": {},  # user_id(str) -> iso
    }


def _load_auth_state() -> dict:
    global _AUTH_STATE
    if _AUTH_STATE is not None:
        return _AUTH_STATE
    with _AUTH_LOCK:
        if _AUTH_STATE is not None:
            return _AUTH_STATE
        state = _auth_state_default()
        try:
            if os.path.exists(AUTH_STATE_PATH):
                with open(AUTH_STATE_PATH, "r", encoding="utf-8") as f:
                    data = json.load(f) or {}
                if isinstance(data, dict):
                    state.update({k: v for k, v in data.items() if k in state and isinstance(v, dict)})
        except Exception:
            logging.getLogger("local_telegram_bot").exception("Failed to load auth state")
        _AUTH_STATE = state
        return state


def _save_auth_state(state: dict) -> None:
    try:
        tmp = AUTH_STATE_PATH + ".tmp"
        with open(tmp, "w", encoding="utf-8") as f:
            json.dump(state, f, ensure_ascii=False, indent=2, sort_keys=True)
        os.replace(tmp, AUTH_STATE_PATH)
    except Exception:
        logging.getLogger("local_telegram_bot").exception("Failed to save auth state")


def _now_utc_iso() -> str:
    return datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc).isoformat()


def _normalize_code(text: str) -> str:
    s = (text or "").strip().lower()
    # Keep letters/numbers and collapse everything else to spaces.
    s = re.sub(r"[^a-z0-9–∞-—è—ë]+", " ", s, flags=re.IGNORECASE).strip()
    s = re.sub(r"\\s+", " ", s).strip()
    return s


def _make_spell_code() -> str:
    rng = secrets.SystemRandom()
    mn, mx = _auth_code_word_count()
    n = rng.randint(mn, mx)
    if len(_SPELL_WORDS) >= n:
        words = rng.sample(_SPELL_WORDS, k=n)
    else:
        words = [rng.choice(_SPELL_WORDS) for _ in range(n)]
    rng.shuffle(words)
    return " ".join(words)


def _auth_is_user_authorized(user_id: int) -> bool:
    st = _load_auth_state()
    return str(int(user_id)) in (st.get("authorized_users") or {})


def _auth_is_chat_whitelisted(chat_id: int) -> bool:
    st = _load_auth_state()
    return str(int(chat_id)) in (st.get("whitelisted_chats") or {})


def _auth_can_use_gemini(user_id: int, chat_id: int) -> bool:
    if not _auth_enabled():
        return True
    return _auth_is_user_authorized(user_id) or _auth_is_chat_whitelisted(chat_id)


def _auth_maybe_whitelist_chat(*, user_id: int, chat_id: int) -> bool:
    """
    If user is authorized and chat isn't whitelisted yet, add it.
    Returns True when chat was newly added.
    """
    if not _auth_enabled():
        return False
    if not _auth_is_user_authorized(user_id):
        return False
    if _auth_is_chat_whitelisted(chat_id):
        return False
    with _AUTH_LOCK:
        st = _load_auth_state()
        if str(int(chat_id)) in st["whitelisted_chats"]:
            return False
        st["whitelisted_chats"][str(int(chat_id))] = {"added_by": int(user_id), "added_at": _now_utc_iso()}
        _save_auth_state(st)
        return True


def _auth_should_prompt_user(user_id: int) -> bool:
    if not _auth_enabled():
        return False
    st = _load_auth_state()
    return str(int(user_id)) not in (st.get("prompted_users") or {})


def _auth_mark_prompted(user_id: int) -> None:
    with _AUTH_LOCK:
        st = _load_auth_state()
        st["prompted_users"][str(int(user_id))] = _now_utc_iso()
        _save_auth_state(st)


def _auth_start_pending(user_id: int, email: str) -> str:
    code = _make_spell_code()
    ttl = _auth_code_ttl_sec()
    expires = datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc) + datetime.timedelta(seconds=ttl)
    with _AUTH_LOCK:
        st = _load_auth_state()
        st["pending"][str(int(user_id))] = {
            "email": email,
            "code": code,
            "expires_at": expires.isoformat(),
            "attempts": 0,
        }
        _save_auth_state(st)
    return code


def _auth_has_pending(user_id: int) -> bool:
    st = _load_auth_state()
    return str(int(user_id)) in (st.get("pending") or {})


def _auth_verify_code(user_id: int, text: str) -> tuple[bool, str]:
    """
    Returns (ok, message). On success also persists the authorization.
    """
    domain = _auth_domain()
    uid = str(int(user_id))
    norm = _normalize_code(text)
    with _AUTH_LOCK:
        st = _load_auth_state()
        pending = (st.get("pending") or {}).get(uid)
        if not pending:
            return False, f"–ù–µ—Ç –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–¥–∞. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –≤–∞—à—É –ø–æ—á—Ç—É –≤–∏–¥–∞ `email@{domain}`."

        expires_at = (pending.get("expires_at") or "").strip()
        try:
            exp = datetime.datetime.fromisoformat(expires_at.replace("Z", "+00:00"))
        except Exception:
            exp = None
        if not exp or exp <= datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc):
            st["pending"].pop(uid, None)
            _save_auth_state(st)
            return False, f"–ö–æ–¥ –∏—Å—Ç—ë–∫. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –≤–∞—à—É –ø–æ—á—Ç—É –≤–∏–¥–∞ `email@{domain}` –µ—â—ë —Ä–∞–∑."

        expected = _normalize_code(pending.get("code") or "")
        if norm != expected:
            pending["attempts"] = int(pending.get("attempts") or 0) + 1
            if pending["attempts"] >= 5:
                st["pending"].pop(uid, None)
                _save_auth_state(st)
                return False, f"–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –ø–æ–ø—ã—Ç–æ–∫. –û—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –≤–∞—à—É –ø–æ—á—Ç—É –≤–∏–¥–∞ `email@{domain}` –µ—â—ë —Ä–∞–∑."
            _save_auth_state(st)
            return False, "–ö–æ–¥ –Ω–µ —Å–æ–≤–ø–∞–ª. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å–ª–æ–≤–∞ –∏ –ø–æ—Ä—è–¥–æ–∫ (–∫–∞–∫ –≤ –ø–∏—Å—å–º–µ) –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â—ë —Ä–∞–∑."

        email = (pending.get("email") or "").strip().lower()
        st["pending"].pop(uid, None)
        st["authorized_users"][uid] = {"email": email, "authorized_at": _now_utc_iso()}
        _save_auth_state(st)
        return True, "‚úÖ –ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–∞. –¢–µ–ø–µ—Ä—å –¥–æ—Å—Ç—É–ø–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–µ—Ä–µ–∑ Gemini."


def _smtp_send_email(*, to_email: str, subject: str, body: str) -> None:
    host = (os.environ.get("SMTP_HOST") or "").strip()
    if not host:
        raise RuntimeError("SMTP_HOST –Ω–µ –∑–∞–¥–∞–Ω")
    port = int((os.environ.get("SMTP_PORT") or "").strip() or "587")
    username = (os.environ.get("SMTP_USERNAME") or "").strip()
    password = (os.environ.get("SMTP_PASSWORD") or "").strip()
    from_email = (os.environ.get("SMTP_FROM") or "").strip() or username
    if not from_email:
        raise RuntimeError("SMTP_FROM –Ω–µ –∑–∞–¥–∞–Ω (–∏ SMTP_USERNAME –ø—É—Å—Ç)")

    use_ssl = _truthy_env("SMTP_SSL", False)
    starttls = _truthy_env("SMTP_STARTTLS", True)

    msg = EmailMessage()
    msg["Subject"] = subject
    msg["From"] = from_email
    msg["To"] = to_email
    msg.set_content(body)

    if use_ssl:
        context = ssl.create_default_context()
        with smtplib.SMTP_SSL(host, port, context=context, timeout=20) as s:
            if username and password:
                s.login(username, password)
            s.send_message(msg)
        return

    with smtplib.SMTP(host, port, timeout=20) as s:
        s.ehlo()
        if starttls:
            context = ssl.create_default_context()
            s.starttls(context=context)
            s.ehlo()
        if username and password:
            s.login(username, password)
        s.send_message(msg)


def _setup_usage_logger() -> Optional[logging.Logger]:
    """
    Separate usage log for sessions (JSON Lines).
    Does not include transcripts/prompts; only metadata and timings.
    """
    global _USAGE_LOGGER
    if _USAGE_LOGGER is not None:
        return _USAGE_LOGGER

    if not _truthy_env("USAGE_LOG_ENABLED", True):
        _USAGE_LOGGER = None
        return None

    path = (os.environ.get("USAGE_LOG_PATH") or "").strip() or DEFAULT_USAGE_LOG_PATH
    try:
        logger = logging.getLogger("usage_sessions")
        logger.setLevel(logging.INFO)
        logger.propagate = False
        handler = logging.FileHandler(path, encoding="utf-8")
        handler.setFormatter(logging.Formatter("%(message)s"))
        logger.addHandler(handler)
        _USAGE_LOGGER = logger
        logging.getLogger("local_telegram_bot").info("Usage log enabled: %s", path)
        return logger
    except Exception:
        logging.getLogger("local_telegram_bot").exception("Failed to set up usage logger")
        _USAGE_LOGGER = None
        return None


def _log_usage_session(payload: dict) -> None:
    logger = _USAGE_LOGGER or logging.getLogger("usage_sessions")
    if not logger.handlers:
        return
    try:
        logger.info(json.dumps(payload, ensure_ascii=False, separators=(",", ":")))
    except Exception:
        # Usage logging must never break bot flow.
        pass


def _wav_duration_sec(wav_path: str) -> float:
    import wave

    try:
        with wave.open(wav_path, "rb") as wf:
            frames = wf.getnframes()
            rate = wf.getframerate() or 0
            if rate <= 0:
                return 0.0
            return float(frames) / float(rate)
    except Exception:
        return 0.0


def _load_stats() -> dict:
    try:
        with open(STATS_PATH, "r", encoding="utf-8") as f:
            return json.load(f) or {}
    except Exception:
        return {}


def _save_stats(data: dict) -> None:
    try:
        with open(STATS_PATH, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception:
        pass


def _get_rtf_est(kind: str) -> float:
    stats = _load_stats()
    try:
        return float(((stats.get("rtf") or {}).get(kind) or 0.0) or 0.0)
    except Exception:
        return 0.0


def _update_rtf_est(kind: str, rtf: float) -> None:
    if not (rtf and rtf > 0):
        return
    stats = _load_stats()
    rtf_map = stats.setdefault("rtf", {})
    prev = float(rtf_map.get(kind) or 0.0) if isinstance(rtf_map.get(kind), (int, float)) else 0.0
    alpha = 0.3
    rtf_map[kind] = (prev * (1 - alpha) + rtf * alpha) if prev > 0 else rtf
    _save_stats(stats)


class _ProgressState:
    def __init__(self, *, audio_sec: float, est_rtf: float) -> None:
        self._lock = threading.Lock()
        self.audio_sec = float(audio_sec or 0.0)
        self.est_rtf = float(est_rtf or 0.0)
        self.processed_sec = 0.0
        self.est_total_sec = max(1.0, self.audio_sec * (self.est_rtf or 3.0))

    def set_processed_sec(self, sec: float) -> None:
        with self._lock:
            self.processed_sec = max(self.processed_sec, float(sec or 0.0))

    def snapshot(self) -> tuple[float, float, float]:
        with self._lock:
            return float(self.audio_sec), float(self.processed_sec), float(self.est_total_sec)

    def update_estimate(self, elapsed_sec: float) -> None:
        with self._lock:
            audio_sec = float(self.audio_sec)
            processed_sec = float(self.processed_sec)
            prev = float(self.est_total_sec)

            if audio_sec > 0 and processed_sec > 0:
                progress = min(processed_sec / audio_sec, 0.995)
                raw = max(elapsed_sec / max(1e-3, progress), elapsed_sec)
                # Smooth to avoid jumping ETA.
                self.est_total_sec = max(elapsed_sec, prev * 0.7 + raw * 0.3)
                return

            # No progress info yet (e.g., model warmup/download) ‚Äî keep a conservative estimate.
            base = audio_sec * (self.est_rtf or 3.0)
            self.est_total_sec = max(elapsed_sec, prev, base)


def _parse_timeout_env(name: str) -> Optional[float]:
    """
    Returns:
    - None when unset/empty (meaning: use dynamic default)
    - 0.0 when set to 0 or negative (meaning: disable timeout)
    - float seconds when set to a positive number
    """
    raw = (os.environ.get(name) or "").strip()
    if not raw:
        return None
    try:
        v = float(raw)
    except Exception:
        return None
    if v <= 0:
        return 0.0
    return v


def _default_whisper_timeout_sec(*, audio_sec: float) -> float:
    """
    Dynamic default timeout for Whisper (seconds).
    The old fixed 240s is too small for long audio; keep a conservative scaling.
    """
    audio_sec = float(audio_sec or 0.0)
    rtf = _get_rtf_est("whisper") or 0.0
    if rtf <= 0:
        # Conservative fallback (CPU medium can be slow on some machines).
        rtf = 0.6
    est = max(60.0, audio_sec * rtf)
    # Give extra slack for warmup/IO and variability.
    return max(300.0, est * 2.0 + 120.0)


def _chunks(text: str, *, limit: int = TELEGRAM_TEXT_LIMIT - 50):
    text = text or ""
    while len(text) > limit:
        cut = text.rfind("\n", 0, limit)
        if cut <= 0:
            cut = limit
        yield text[:cut].rstrip()
        text = text[cut:].lstrip()
    if text.strip():
        yield text


def _cancel_markup(session_id: str) -> InlineKeyboardMarkup:
    return InlineKeyboardMarkup(
        [[InlineKeyboardButton("–û—Ç–º–µ–Ω–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫—É", callback_data=f"cancel:{session_id}")]]
    )


class _UserCancelled(Exception):
    pass


def _active_sessions(context: ContextTypes.DEFAULT_TYPE) -> dict:
    return context.application.bot_data.setdefault("active_sessions", {})


def _ru_plural(n: int, one: str, few: str, many: str) -> str:
    n = abs(int(n))
    if (n % 10) == 1 and (n % 100) != 11:
        return one
    if 2 <= (n % 10) <= 4 and not (12 <= (n % 100) <= 14):
        return few
    return many


def _active_sessions_summary_line(context: ContextTypes.DEFAULT_TYPE) -> str:
    sessions = _active_sessions(context) or {}
    files = len(sessions)
    if files <= 0:
        return ""

    user_keys = set()
    for sid, sess in sessions.items():
        if not isinstance(sess, dict):
            user_keys.add(("s", sid))
            continue
        uid = sess.get("user_id")
        if uid:
            user_keys.add(("u", int(uid)))
        else:
            user_keys.add(("s", sid))

    users = len(user_keys)
    file_word = _ru_plural(files, "—Ñ–∞–π–ª", "—Ñ–∞–π–ª–∞", "—Ñ–∞–π–ª–æ–≤")
    # Genitive after "–æ—Ç":
    user_word = _ru_plural(users, "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è", "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π", "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
    return f"–í –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {files} {file_word} –æ—Ç {users} {user_word}."


def _encode_audio_preview_ffmpeg(
    *,
    ffmpeg_bin: str,
    src_wav: str,
    out_dir: str,
    base_name: str = "processed_audio",
) -> str:
    """
    Best-effort: encodes wav to a compact format for sending back to user.
    Tries MP3 first, then OGG/Opus.
    Returns output file path.
    """
    ffmpeg_bin = (ffmpeg_bin or "ffmpeg").strip() or "ffmpeg"
    os.makedirs(out_dir, exist_ok=True)

    mp3_path = os.path.join(out_dir, f"{base_name}.mp3")
    cmd_mp3 = [
        ffmpeg_bin,
        "-y",
        "-i",
        src_wav,
        "-vn",
        "-ac",
        "1",
        "-ar",
        "16000",
        "-c:a",
        "libmp3lame",
        "-b:a",
        "64k",
        mp3_path,
    ]
    r = subprocess.run(cmd_mp3, capture_output=True, text=True)
    if r.returncode == 0 and os.path.exists(mp3_path):
        return mp3_path

    ogg_path = os.path.join(out_dir, f"{base_name}.ogg")
    cmd_ogg = [
        ffmpeg_bin,
        "-y",
        "-i",
        src_wav,
        "-vn",
        "-ac",
        "1",
        "-ar",
        "16000",
        "-c:a",
        "libopus",
        "-b:a",
        "32k",
        "-vbr",
        "on",
        "-compression_level",
        "10",
        ogg_path,
    ]
    r2 = subprocess.run(cmd_ogg, capture_output=True, text=True)
    if r2.returncode == 0 and os.path.exists(ogg_path):
        return ogg_path

    raise RuntimeError(
        "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏.\n"
        f"MP3 error: {(r.stderr or '').strip()}\n"
        f"OGG error: {(r2.stderr or '').strip()}"
    )


async def _gigaam_transcribe_hard_cancel(
    *,
    wav_path: str,
    model_name: str,
    device: str,
    hf_token: Optional[str],
    cancel_event: asyncio.Event,
) -> tuple[list, dict]:
    """
    Runs GigaAM in a separate process and terminates it if cancel_event is set.
    Returns (segments, info) where segments is a list of ASRSegment-like tuples.
    """
    ctx = multiprocessing.get_context("spawn")
    q = ctx.Queue(maxsize=1)
    proc = ctx.Process(
        target=gigaam_transcribe_worker,
        kwargs={
            "wav_path": wav_path,
            "model_name": model_name,
            "device": device,
            "hf_token": hf_token,
            "out_queue": q,
        },
        daemon=True,
    )
    proc.start()

    async def _wait_result():
        return await asyncio.to_thread(q.get)

    wait_task = asyncio.create_task(_wait_result())
    cancel_task = asyncio.create_task(cancel_event.wait())
    try:
        done, pending = await asyncio.wait({wait_task, cancel_task}, return_when=asyncio.FIRST_COMPLETED)
        if cancel_task in done:
            try:
                if proc.is_alive():
                    proc.terminate()
            except Exception:
                pass
            try:
                await asyncio.to_thread(proc.join, 2.0)
            except Exception:
                pass
            raise _UserCancelled()

        payload = wait_task.result()
        try:
            await asyncio.to_thread(proc.join, 2.0)
        except Exception:
            pass

        if not isinstance(payload, dict):
            raise RuntimeError("GigaAM worker returned invalid payload.")
        if not payload.get("ok"):
            err_type = str(payload.get("error_type") or "GigaAMError")
            err = str(payload.get("error") or "").strip() or err_type
            raise RuntimeError(err)

        segs = payload.get("segments") or []
        info = payload.get("info") or {}
        return segs, info
    finally:
        for t in (wait_task, cancel_task):
            if not t.done():
                t.cancel()
        try:
            if proc.is_alive():
                proc.terminate()
        except Exception:
            pass


async def cb_cancel(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    q = getattr(update, "callback_query", None)
    if not q or not getattr(q, "data", None):
        return
    data = str(q.data)
    if not data.startswith("cancel:"):
        return

    session_id = data.split(":", 1)[1].strip()
    sess = (_active_sessions(context) or {}).get(session_id)
    if not sess:
        try:
            await q.answer("–°–µ—Å—Å–∏—è —É–∂–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.", show_alert=False)
        except Exception:
            pass
        return

    ev = sess.get("cancel_event")
    if isinstance(ev, asyncio.Event):
        ev.set()

    try:
        await q.answer("–û–∫, –æ—Ç–º–µ–Ω—è—é‚Ä¶", show_alert=False)
    except Exception:
        pass

    # Best-effort update the status message itself (button will be removed).
    try:
        await q.message.edit_text("‚õîÔ∏è –û—Ç–º–µ–Ω–∞ –∑–∞–ø—Ä–æ—à–µ–Ω–∞ ‚Äî –∑–∞–≤–µ—Ä—à–∞—é —Ç–µ–∫—É—â–∏–π —à–∞–≥‚Ä¶", reply_markup=None)
    except Exception:
        pass


async def _reply_long(update: Update, text: str) -> None:
    for part in _chunks(text):
        await update.effective_message.reply_text(part)


async def _safe_edit(message, text: str) -> Optional[float]:
    try:
        if not message:
            return None
        await message.edit_text(text)
        return None
    except RetryAfter as exc:
        return float(getattr(exc, "retry_after", 0) or 0) or 1.0
    except Exception:
        # Message can be deleted/edited too often/etc. Don't fail ASR because of UI.
        return None


async def _safe_edit_formatted(message, text: str, *, parse_mode: Optional[str] = None) -> Optional[float]:
    """
    Like `_safe_edit`, but allows setting `parse_mode` (HTML/MarkdownV2).
    """
    try:
        if not message:
            return None
        await message.edit_text(text, parse_mode=parse_mode)
        return None
    except RetryAfter as exc:
        return float(getattr(exc, "retry_after", 0) or 0) or 1.0
    except Exception:
        return None


async def _safe_edit_message(
    message,
    text: str,
    *,
    parse_mode: Optional[str] = None,
    reply_markup=None,
) -> Optional[float]:
    """
    Like `_safe_edit`, but supports parse_mode and reply_markup (inline keyboards).
    """
    try:
        if not message:
            return None
        await message.edit_text(text, parse_mode=parse_mode, reply_markup=reply_markup)
        return None
    except RetryAfter as exc:
        return float(getattr(exc, "retry_after", 0) or 0) or 1.0
    except Exception:
        return None


async def _safe_delete(message) -> None:
    try:
        if message:
            await message.delete()
    except Exception:
        pass


async def _ticker(
    *,
    context: ContextTypes.DEFAULT_TYPE,
    chat_id: int,
    message,
    base_text: str,
    reply_markup=None,
    state: Optional[_ProgressState] = None,
    cancel_event: Optional[asyncio.Event] = None,
    interval_sec: float = 5.0,
) -> None:
    start = time.monotonic()
    try:
        while True:
            if cancel_event is not None and cancel_event.is_set():
                return
            elapsed_sec = time.monotonic() - start
            if state is not None:
                state.update_estimate(elapsed_sec)
                audio_sec, processed_sec, est_total_sec = state.snapshot()
                if audio_sec > 0 and processed_sec > 0:
                    pct = int(min(99, (processed_sec / audio_sec) * 100))
                    line = f"‚è± {_fmt_dur(elapsed_sec)}/{_fmt_dur(est_total_sec)} ‚Ä¢ {pct}%"
                else:
                    line = f"‚è± {_fmt_dur(elapsed_sec)}/{_fmt_dur(est_total_sec)}"
            else:
                line = f"‚è± {_fmt_dur(elapsed_sec)}"

            summary = _active_sessions_summary_line(context)
            text = f"{base_text}\n{line}"
            if summary:
                text += f"\n{summary}"
            retry_after = await _safe_edit_message(message, text, reply_markup=reply_markup)
            try:
                await context.bot.send_chat_action(chat_id=chat_id, action=ChatAction.TYPING)
            except RetryAfter as exc:
                retry_after = max(retry_after or 0.0, float(getattr(exc, "retry_after", 0) or 0) or 1.0)
            except Exception:
                pass

            sleep_for = interval_sec
            if retry_after is not None and retry_after > 0:
                sleep_for = max(sleep_for, retry_after + 0.5)
            await asyncio.sleep(sleep_for)
    except asyncio.CancelledError:
        return


def _get_env(name: str, default: str) -> str:
    v = (os.environ.get(name) or "").strip()
    return v if v else default


async def cmd_start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    _ = context
    domain = _auth_domain()
    intro = (
        "–î–æ–±—Ä–æ –ø–æ–∂–∞–ª–æ–≤–∞—Ç—å –≤ –∏–¥–µ–∞–ª—å–Ω—ã–π (–Ω—É –ø–æ—á—Ç–∏ :)) —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ç–æ—Ä —Ä—É—Å—Å–∫–∏—Ö –∞—É–¥–∏–æ —Å–æ–æ–±—â–µ–Ω–∏–π. "
        "–Ø –∏—Å–ø–æ–ª—å–∑—É—é –¥–ª—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –¥–≤–µ –º–æ–¥–µ–ª–∏: Whisper –∏ GigaAM, "
        "–∞ –∑–∞—Ç–µ–º —Å –ø–æ–º–æ—â—å—é LLM –¥–µ–ª–∞—é –∏—Ö —Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∏ –∏—Ç–æ–≥–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏.\n\n"
        "–ë–æ—Ç —Ö–æ—Ä–æ—à–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –∑–∞–¥–∞—á, –∫–æ–≥–¥–∞ –≤–∞–∂–Ω–∞ —Ç–æ—á–Ω–æ—Å—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏, –∞ –Ω–µ —Å–∫–æ—Ä–æ—Å—Ç—å: "
        "–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–æ—Å—Ç–æ–≤ –¥–ª—è social media, –¥–æ–ª–∂–Ω–æ—Å—Ç–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–π –∏ —Ç.–ø.\n\n"
    )

    if _auth_enabled():
        auth_block = (
            "–ï—Å–ª–∏ –≤—ã —Å–æ—Ç—Ä—É–¥–Ω–∏–∫ Business Booster, –∞–≤—Ç–æ—Ä–∏–∑—É–π—Ç–µ—Å—å –ø–æ –∫–æ—Ä–ø–æ—Ä–∞—Ç–∏–≤–Ω–æ–π –ø–æ—á—Ç–µ, "
            "—Ç–æ–≥–¥–∞ –≤–∞–º –±—É–¥–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–∞ –±–æ–ª–µ–µ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–∞—è –ø–ª–∞—Ç–Ω–∞—è –º–æ–¥–µ–ª—å Gemini 3 Pro.\n\n"
            f"–ü—Ä–æ—Å—Ç–æ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –≤–∞—à—É –ø–æ—á—Ç—É –≤–∏–¥–∞ `email@{domain}`.\n\n"
            "–ï—Å–ª–∏ –≤—ã –≥–æ—Å—Ç—å, —Ç–æ –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ—Ç –±–µ—Å–ø–ª–∞—Ç–Ω–æ –Ω–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ "
            "(—ç—Ç–æ –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ –∏ —á—É—Ç—å –º–µ–Ω–µ–µ —Ç–æ—á–Ω–æ, –Ω–æ —Ç–æ–∂–µ –Ω–µ–ø–ª–æ—Ö–æ)."
        )
    else:
        auth_block = (
            "–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ (AUTH_ENABLED=0). "
            "–ë–æ—Ç –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –¥–æ—Å—Ç—É–ø–Ω–æ–π LLM –±–µ–∑ –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–æ–º–µ–Ω–∞."
        )

    await update.effective_message.reply_text(intro + auth_block)


async def cmd_auth(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    msg = update.effective_message
    user = update.effective_user
    if msg is None or user is None:
        return

    user_id = int(user.id)
    if _auth_is_user_authorized(user_id):
        await msg.reply_text("‚úÖ –í—ã —É–∂–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã.")
        return

    if not _auth_enabled():
        await msg.reply_text("–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ (AUTH_ENABLED=0). –ù–∏—á–µ–≥–æ –¥–µ–ª–∞—Ç—å –Ω–µ –Ω—É–∂–Ω–æ.")
        return

    domain = _auth_domain()
    email = (" ".join(context.args or [])).strip()
    if not email:
        await msg.reply_text(f"–û—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ –≤–∞—à—É –ø–æ—á—Ç—É –≤–∏–¥–∞ `email@{domain}`.")
        return

    email = email.strip().lower()
    # Minimal email validation (enough for domain-gated auth).
    # NOTE: must use `\s` (whitespace), not `\\s` (literal "s" + backslash in a char-class).
    if not re.match(r"^[^@\s]+@[^@\s]+\.[^@\s]+$", email):
        await msg.reply_text(f"–ù–µ–≤–µ—Ä–Ω—ã–π email. –ü—Ä–∏–º–µ—Ä: name@{domain}")
        return
    if not email.endswith(f"@{domain}"):
        await msg.reply_text(f"–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}.")
        return

    code = _auth_start_pending(user_id, email)
    ttl_min = max(1, int(_auth_code_ttl_sec() / 60))

    subject = "Ideal Russian Transcriber ‚Äî –∫–æ–¥ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"
    body = (
        "–ß—Ç–æ–±—ã –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å—Å—è –≤ –±–æ—Ç–µ, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –≤ Telegram —Å–ª–µ–¥—É—é—â—É—é —Ñ—Ä–∞–∑—É (—Å–ª–æ–≤–∞ –∏ –ø–æ—Ä—è–¥–æ–∫ –≤–∞–∂–Ω—ã):\n\n"
        f"{code}\n\n"
        f"–ö–æ–¥ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω {ttl_min} –º–∏–Ω—É—Ç."
    )

    await msg.reply_text(f"üìß –û—Ç–ø—Ä–∞–≤–ª—è—é –∫–æ–¥ –Ω–∞ {email}‚Ä¶")
    loop = asyncio.get_running_loop()
    try:
        await loop.run_in_executor(None, lambda: _smtp_send_email(to_email=email, subject=subject, body=body))
    except Exception as exc:
        logging.getLogger("local_telegram_bot").exception("Failed to send auth email")
        await msg.reply_text(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –ø–∏—Å—å–º–æ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ SMTP –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ `.env` "
            "(SMTP_HOST/SMTP_PORT/SMTP_USERNAME/SMTP_PASSWORD/SMTP_FROM).\n"
            f"–û—à–∏–±–∫–∞: {exc}"
        )
        return

    mn, mx = _auth_code_word_count()
    await msg.reply_text(f"‚úÖ –ü–∏—Å—å–º–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ {email}.\n–ü—Ä–∏—à–ª–∏—Ç–µ —Å—é–¥–∞ –∫–æ–¥ –∏–∑ {mn}-{mx} —Å–ª–æ–≤ (–∫–∞–∫ –≤ –ø–∏—Å—å–º–µ).")


async def _begin_email_auth(update: Update, context: ContextTypes.DEFAULT_TYPE, *, email: str) -> None:
    """
    Starts email authorization flow from a plain email message (no /auth command required).
    """
    msg = update.effective_message
    user = update.effective_user
    if msg is None or user is None:
        return

    if not _auth_enabled():
        await msg.reply_text("–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∞ (AUTH_ENABLED=0).")
        return

    user_id = int(user.id)
    domain = _auth_domain()
    email = (email or "").strip().lower()

    if _auth_is_user_authorized(user_id):
        await msg.reply_text("‚úÖ –í—ã —É–∂–µ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω—ã.")
        return

    if not re.match(r"^[^@\s]+@[^@\s]+\.[^@\s]+$", email):
        await msg.reply_text(f"–ù–µ–≤–µ—Ä–Ω—ã–π email. –ü—Ä–∏–º–µ—Ä: name@{domain}")
        return
    if not email.endswith(f"@{domain}"):
        await msg.reply_text(f"–ê–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–æ—Å—Ç—É–ø–Ω–∞ —Ç–æ–ª—å–∫–æ –¥–ª—è –¥–æ–º–µ–Ω–∞ {domain}.")
        return

    code = _auth_start_pending(user_id, email)
    ttl_min = max(1, int(_auth_code_ttl_sec() / 60))

    subject = "Ideal Russian Transcriber ‚Äî –∫–æ–¥ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"
    body = (
        "–ß—Ç–æ–±—ã –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å—Å—è –≤ –±–æ—Ç–µ, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –≤ Telegram —Å–ª–µ–¥—É—é—â—É—é —Ñ—Ä–∞–∑—É (—Å–ª–æ–≤–∞ –∏ –ø–æ—Ä—è–¥–æ–∫ –≤–∞–∂–Ω—ã):\n\n"
        f"{code}\n\n"
        f"–ö–æ–¥ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª–µ–Ω {ttl_min} –º–∏–Ω—É—Ç."
    )

    await msg.reply_text(f"üìß –û—Ç–ø—Ä–∞–≤–ª—è—é –∫–æ–¥ –Ω–∞ {email}‚Ä¶")
    loop = asyncio.get_running_loop()
    try:
        await loop.run_in_executor(None, lambda: _smtp_send_email(to_email=email, subject=subject, body=body))
    except Exception as exc:
        logging.getLogger("local_telegram_bot").exception("Failed to send auth email")
        await msg.reply_text(
            "–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –ø–∏—Å—å–º–æ. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ SMTP –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ `.env` "
            "(SMTP_HOST/SMTP_PORT/SMTP_USERNAME/SMTP_PASSWORD/SMTP_FROM).\n"
            f"–û—à–∏–±–∫–∞: {exc}"
        )
        return

    mn, mx = _auth_code_word_count()
    await msg.reply_text(f"‚úÖ –ü–∏—Å—å–º–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –Ω–∞ {email}.\n–ü—Ä–∏—à–ª–∏—Ç–µ —Å—é–¥–∞ –∫–æ–¥ –∏–∑ {mn}-{mx} —Å–ª–æ–≤ (–∫–∞–∫ –≤ –ø–∏—Å—å–º–µ).")


def _pick_telegram_file(update: Update):
    msg = update.effective_message
    if msg is None:
        return None, None, None
    return _pick_telegram_file_from_message(msg)


def _pick_telegram_file_from_message(msg):
    if msg.voice:
        return msg.voice.file_id, "voice.ogg", msg.voice.file_size
    if msg.audio:
        name = msg.audio.file_name or "audio"
        return msg.audio.file_id, name, msg.audio.file_size
    if msg.document and (msg.document.mime_type or "").startswith("audio/"):
        name = msg.document.file_name or "audio.bin"
        return msg.document.file_id, name, msg.document.file_size

    return None, None, None


def _telegram_max_get_file_bytes() -> int:
    """
    Telegram Bot API download limit precheck (in MB).
    Set TELEGRAM_MAX_GET_FILE_MB=0 to disable precheck.
    """
    raw = (os.environ.get("TELEGRAM_MAX_GET_FILE_MB") or "").strip()
    mb = int(raw or "20")
    if mb <= 0:
        return 0
    return mb * 1024 * 1024


def _file_too_big_message(*, size_mb: float, limit_mb: float) -> str:
    """
    User-facing guidance for speech audio compression.
    """
    size_mb = max(0.0, float(size_mb))
    limit_mb = max(0.0, float(limit_mb)) or 20.0

    # Rough estimates: MB/min ‚âà (kbps * 60) / 8 / 1024
    mp3_kbps = 48
    ogg_kbps = 24
    mp3_mb_per_min = (mp3_kbps * 60) / 8 / 1024
    ogg_mb_per_min = (ogg_kbps * 60) / 8 / 1024
    mp3_max_min = int(limit_mb / mp3_mb_per_min) if mp3_mb_per_min > 0 else 0
    ogg_max_min = int(limit_mb / ogg_mb_per_min) if ogg_mb_per_min > 0 else 0

    return (
        f"–û—à–∏–±–∫–∞: —Ñ–∞–π–ª —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –±–æ—Ç–æ–º —á–µ—Ä–µ–∑ Telegram API "
        f"({size_mb:.1f}MB, –ª–∏–º–∏—Ç ~{limit_mb:.0f}MB). "
        f"–°–æ–∂–º–∏—Ç–µ —Ñ–∞–π–ª, —á—Ç–æ–±—ã –æ–Ω —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è–ª —ç—Ç–æ–º—É –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—é –∏ –æ—Ç–ø—Ä–∞–≤—å—Ç–µ —Å–Ω–æ–≤–∞.\n\n"
        f"–ö–∞–∫ —Å–∂–∞—Ç—å –≥–æ–ª–æ—Å (ffmpeg):\n"
        f"1) MP3 (—Ö–æ—Ä–æ—à–µ–µ –∫–∞—á–µ—Å—Ç–≤–æ —Ä–µ—á–∏):\n"
        f"   ffmpeg -i input.mp3 -vn -ac 1 -ar 24000 -b:a {mp3_kbps}k output.mp3\n"
        f"   ~{mp3_mb_per_min:.2f}MB/–º–∏–Ω (‚âà –¥–æ {mp3_max_min} –º–∏–Ω –≤ {limit_mb:.0f}MB)\n"
        f"2) OGG/Opus (–æ–±—ã—á–Ω–æ –º–µ–Ω—å—à–µ, –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è voice):\n"
        f"   ffmpeg -i input.mp3 -vn -c:a libopus -application voip -vbr on -ac 1 -b:a {ogg_kbps}k output.ogg\n"
        f"   ~{ogg_mb_per_min:.2f}MB/–º–∏–Ω (‚âà –¥–æ {ogg_max_min} –º–∏–Ω –≤ {limit_mb:.0f}MB)\n"
    )


async def _send_transcript_file(update: Update, *, whisper_text: str, gigaam_text: str) -> None:
    msg = update.effective_message
    if msg is None:
        return

    now = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    chat_id = update.effective_chat.id if update.effective_chat else "chat"
    filename = f"transcript_{chat_id}_{now}.txt"

    content = (
        "Whisper:\n"
        f"{(whisper_text or '').strip()}\n\n"
        "GigaAM:\n"
        f"{(gigaam_text or '').strip()}\n"
    )
    data = content.encode("utf-8")
    bio = io.BytesIO(data)
    bio.name = filename
    await msg.reply_document(document=bio, filename=filename)


async def _send_markdown_file(
    update: Update,
    *,
    final_text: str,
    final_error: str,
    final_label: str,
    whisper_text: str,
    gigaam_text: str,
) -> None:
    msg = update.effective_message
    if msg is None:
        return

    now = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    chat_id = update.effective_chat.id if update.effective_chat else "chat"
    filename = f"transcript_{chat_id}_{now}.md"

    if final_error and not final_text:
        final_block = (
            "### –ò—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç:\n\n"
            f"_({final_label} –Ω–µ —Å–º–æ–≥ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å –∏—Ç–æ–≥)_\n\n"
            "### –ü—Ä–∏–º–µ—á–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä–∞:\n\n"
            "_(–Ω–µ—Ç)_\n\n"
            "### –û—Ç—á–µ—Ç –æ –ø—Ä–æ–¥–µ–ª–∞–Ω–Ω—ã—Ö –¥–µ–π—Å—Ç–≤–∏—è—Ö:\n\n"
            f"- –û—à–∏–±–∫–∞: `{final_error.strip()}`\n"
        )
    else:
        final_block = (final_text or "").strip()

    content = (
        f"## 1) –ò—Ç–æ–≥–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç –ø–æ —à–∞–±–ª–æ–Ω—É ({final_label})\n\n"
        f"{final_block}\n\n"
        "---\n\n"
        "## 2) –í–∞—Ä–∏–∞–Ω—Ç Whisper\n\n"
        f"{(whisper_text or '').strip()}\n\n"
        "---\n\n"
        "## 3) –í–∞—Ä–∏–∞–Ω—Ç GigaAM\n\n"
        f"{(gigaam_text or '').strip()}\n"
    )
    bio = io.BytesIO(content.encode("utf-8"))
    bio.name = filename
    await msg.reply_document(document=bio, filename=filename)


def _parse_float_env(value: str, default: float) -> float:
    try:
        return float((value or "").strip())
    except Exception:
        return float(default)


def _parse_int_env(value: str, default: int) -> int:
    try:
        return int((value or "").strip())
    except Exception:
        return int(default)


def _thinking_level_value(level: str) -> str:
    v = (level or "").strip().lower()
    # Gemini API expects plain strings ("high"/"low"/"THINKING_LEVEL_UNSPECIFIED") for v1beta.
    if v in {"high", "h"}:
        return "high"
    if v in {"medium", "mid", "m"}:
        return "medium"
    if v in {"low", "l"}:
        return "low"
    return "THINKING_LEVEL_UNSPECIFIED"


def _media_resolution_value(value: str) -> str:
    v = (value or "").strip().lower()
    # Gemini API accepts enum-like strings for mediaResolution; keep mapping for future multimodal use.
    if v in {"default", "unspecified", ""}:
        return "MEDIA_RESOLUTION_UNSPECIFIED"
    if v in {"low"}:
        return "MEDIA_RESOLUTION_LOW"
    if v in {"medium", "mid"}:
        return "MEDIA_RESOLUTION_MEDIUM"
    if v in {"high"}:
        return "MEDIA_RESOLUTION_HIGH"
    return "MEDIA_RESOLUTION_UNSPECIFIED"


async def _gemini_generate(
    *,
    api_key: str,
    model: str,
    prompt: str,
    system_prompt: Optional[str],
    generation_config: dict,
) -> str:
    url = f"{GEMINI_API_URL}/models/{model}:generateContent"
    params = {"key": api_key}
    payload = {
        "systemInstruction": {"role": "system", "parts": [{"text": system_prompt}]} if system_prompt else None,
        "contents": [{"role": "user", "parts": [{"text": prompt}]}],
        "generationConfig": generation_config,
    }
    payload = {k: v for k, v in payload.items() if v is not None}

    timeout = httpx.Timeout(connect=10.0, read=240.0, write=30.0, pool=30.0)
    async with httpx.AsyncClient(timeout=timeout) as client:
        r = await client.post(url, params=params, json=payload)
        r.raise_for_status()
        data = r.json()

    candidates = data.get("candidates") or []
    if not candidates:
        raise RuntimeError(f"Gemini: empty response: {data!r}")
    content = (candidates[0] or {}).get("content") or {}
    parts = content.get("parts") or []
    texts = []
    for p in parts:
        t = (p or {}).get("text")
        if t:
            texts.append(str(t))
    out = "\n".join(texts).strip()
    return out


def _trim_for_telegram(text: str, *, limit: int = TELEGRAM_TEXT_LIMIT - 50) -> str:
    text = (text or "").strip()
    if len(text) <= limit:
        return text
    # Keep the tail (usually most relevant in a live "thoughts" stream).
    return "‚Ä¶\n" + text[-limit:].lstrip()


def _markdown_bold_lines_to_html(text: str) -> str:
    """
    Minimal Markdown -> HTML conversion for lines like "**Header**" so Telegram shows them bold.
    Everything else is HTML-escaped (no other markdown is interpreted).
    """
    out_lines = []
    for line in (text or "").splitlines():
        s = line.strip()
        if s.startswith("**") and s.endswith("**") and len(s) >= 4 and s.count("**") == 2:
            inner = s[2:-2].strip()
            out_lines.append(f"<b>{_html.escape(inner)}</b>")
        else:
            out_lines.append(_html.escape(line))
    return "\n".join(out_lines).strip()


def _markdown_to_telegram_html(text: str) -> str:
    """
    Minimal Markdown -> Telegram HTML:
    - Headings like '### Title' become bold lines.
    - Inline **bold** becomes <b>bold</b> (within a single line).
    Everything else is HTML-escaped.
    """

    def md_line_to_html(line: str) -> str:
        raw = line.rstrip("\n")
        stripped = raw.strip()

        # Headings: "#", "##", ... "######"
        m = re.match(r"^(#{1,6})\\s+(.*)$", stripped)
        if m:
            title = m.group(2).strip()
            return f"<b>{_html.escape(title)}</b>"

        # Inline bold: **...** (same line only)
        parts = raw.split("**")
        # Even length => unmatched ** markers; keep as plain text.
        if len(parts) >= 3 and (len(parts) % 2 == 1):
            out = []
            for i, part in enumerate(parts):
                if i % 2 == 1:
                    out.append(f"<b>{_html.escape(part)}</b>")
                else:
                    out.append(_html.escape(part))
            return "".join(out)

        return _html.escape(raw)

    lines = (text or "").splitlines()
    return "\n".join(md_line_to_html(line) for line in lines).strip()


def _split_html_for_telegram(html_text: str, *, limit: int = TELEGRAM_TEXT_LIMIT - 50):
    """
    Splits HTML text into chunks that Telegram can accept, trying to avoid
    cutting inside tags and keeping <b> tags balanced across chunks.
    """
    s = (html_text or "").strip()
    if not s:
        return

    open_bold = False
    while s:
        if len(s) <= limit:
            chunk = s
            s = ""
        else:
            # Prefer cutting at a newline or space before the limit.
            cut = max(s.rfind("\n", 0, limit), s.rfind(" ", 0, limit))
            if cut <= 0:
                cut = limit

            # Avoid cutting inside an HTML tag.
            sub = s[:cut]
            lt = sub.rfind("<")
            gt = sub.rfind(">")
            if lt > gt:
                # We're inside a tag; back up to before the tag.
                cut = lt
                if cut <= 0:
                    cut = limit

            chunk = s[:cut].rstrip()
            s = s[cut:].lstrip()

        # Track bold tag state and balance within the chunk.
        opens = chunk.count("<b>")
        closes = chunk.count("</b>")
        if open_bold:
            chunk = "<b>" + chunk
            open_bold = False
            opens += 1
        if opens > closes:
            open_bold = True
            chunk = chunk + "</b>"

        if chunk.strip():
            yield chunk


async def _reply_long_html(update: Update, html_text: str) -> None:
    for part in _split_html_for_telegram(html_text):
        await update.effective_message.reply_text(part, parse_mode=ParseMode.HTML)


def _gemini_chat_excerpt(text: str) -> str:
    """
    For chat output, show only the final text section (before notes/actions).
    Full output always goes to the markdown file.
    """
    s = (text or "").strip()
    if not s:
        return ""
    markers = [
        "### –ü—Ä–∏–º–µ—á–∞–Ω–∏—è –¥–ª—è –∫–æ–Ω—Ç–µ–Ω—Ç-–º–µ–Ω–µ–¥–∂–µ—Ä–∞",
        "### –ü—Ä–∏–º–µ—á–∞–Ω–∏—è",
    ]
    for m in markers:
        idx = s.find(m)
        if idx != -1:
            return s[:idx].rstrip()
    return s


async def _gemini_stream_generate(
    *,
    api_key: str,
    model: str,
    prompt: str,
    system_prompt: Optional[str],
    generation_config: dict,
    on_update,
    cancel_event: Optional[asyncio.Event] = None,
) -> str:
    """
    Streams `:streamGenerateContent` and calls `on_update(thoughts, elapsed_sec)` periodically.
    Returns final (non-thought) text.
    """
    url = f"{GEMINI_API_URL}/models/{model}:streamGenerateContent"
    # Gemini supports SSE streaming via `alt=sse`.
    params = {"key": api_key, "alt": "sse"}
    payload = {
        "systemInstruction": {"role": "system", "parts": [{"text": system_prompt}]} if system_prompt else None,
        "contents": [{"role": "user", "parts": [{"text": prompt}]}],
        "generationConfig": generation_config,
    }
    payload = {k: v for k, v in payload.items() if v is not None}

    timeout = httpx.Timeout(connect=10.0, read=600.0, write=30.0, pool=30.0)
    thoughts = ""
    final = ""
    finish_reason: Optional[str] = None
    started = time.monotonic()
    last_ui = 0.0
    ui_interval = 3.0

    async with httpx.AsyncClient(timeout=timeout) as client:
        async with client.stream(
            "POST",
            url,
            params=params,
            json=payload,
            headers={"Accept": "text/event-stream"},
        ) as r:
            r.raise_for_status()
            async for raw_line in r.aiter_lines():
                if cancel_event is not None and cancel_event.is_set():
                    raise _UserCancelled()
                line = (raw_line or "").strip()
                if not line:
                    continue
                if line.startswith("data:"):
                    line = line[len("data:") :].strip()
                if line == "[DONE]":
                    break

                try:
                    chunk = json.loads(line)
                except Exception:
                    continue

                candidates = chunk.get("candidates") or []
                if not candidates:
                    continue
                cand0 = candidates[0] or {}
                finish_reason = cand0.get("finishReason") or finish_reason
                content = (cand0 or {}).get("content") or {}
                parts = content.get("parts") or []
                for p in parts:
                    t = (p or {}).get("text")
                    if not t:
                        continue
                    if (p or {}).get("thought") is True:
                        thoughts += str(t)
                    else:
                        final += str(t)

                now = time.monotonic()
                if (now - last_ui) >= ui_interval:
                    last_ui = now
                    try:
                        await on_update(thoughts, now - started)
                    except Exception:
                        pass

    try:
        await on_update(thoughts, time.monotonic() - started)
    except Exception:
        pass
    out = (final or "").strip()
    if not out:
        if finish_reason == "MAX_TOKENS":
            raise RuntimeError(
                "Gemini –≤–µ—Ä–Ω—É–ª —Ç–æ–ª—å–∫–æ thinking-—á–∞—Å—Ç—å –∏ —É–ø—ë—Ä—Å—è –≤ –ª–∏–º–∏—Ç —Ç–æ–∫–µ–Ω–æ–≤ (MAX_TOKENS), –ø–æ—ç—Ç–æ–º—É —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç.\n"
                "–£–≤–µ–ª–∏—á—å—Ç–µ `GEMINI_MAX_OUTPUT_TOKENS` –∏–ª–∏ —É–º–µ–Ω—å—à–∏—Ç–µ –ø—Ä–æ–º–ø—Ç/—Å–ª–æ–∂–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏."
            )
        raise RuntimeError("Gemini –Ω–µ –≤–µ—Ä–Ω—É–ª —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø—É—Å—Ç–æ).")
    return out


def _ollama_base_url() -> str:
    return (os.environ.get("OLLAMA_BASE_URL") or DEFAULT_OLLAMA_BASE_URL).strip().rstrip("/")


def _ollama_pick_best_local_model() -> str:
    """
    Best-effort: choose the most capable *locally available* Ollama model.
    Prefers larger models and skips embedding-only / remote models.
    """
    base_url = _ollama_base_url()
    try:
        r = httpx.get(f"{base_url}/api/tags", timeout=10.0)
        r.raise_for_status()
        data = r.json() or {}
    except Exception:
        return ""

    models = data.get("models") or []
    if not isinstance(models, list):
        return ""

    def is_embedding(name: str) -> bool:
        n = (name or "").lower()
        return any(
            t in n
            for t in (
                "embedding",
                "embed",
                "text-embedding",
                "nomic-embed",
                "mxbai-embed",
                "bge-",
            )
        )

    candidates = []
    for m in models:
        if not isinstance(m, dict):
            continue
        # Exclude cloud/remote entries: user asked for local only.
        if (m.get("remote_host") or "").strip():
            continue
        name = (m.get("name") or m.get("model") or "").strip()
        if not name:
            continue
        if is_embedding(name):
            continue
        size = int(m.get("size") or 0)
        candidates.append((size, name))

    if not candidates:
        # Fallback: any local model.
        for m in models:
            if not isinstance(m, dict):
                continue
            if (m.get("remote_host") or "").strip():
                continue
            name = (m.get("name") or m.get("model") or "").strip()
            if not name:
                continue
            size = int(m.get("size") or 0)
            candidates.append((size, name))

    if not candidates:
        return ""
    candidates.sort()
    return candidates[-1][1]


def _ollama_model() -> str:
    raw = (os.environ.get("OLLAMA_MODEL") or "").strip()
    if not raw or raw.lower() == "auto":
        picked = _ollama_pick_best_local_model()
        return picked or DEFAULT_OLLAMA_MODEL
    return raw


def _ollama_options_from_env() -> dict:
    # Keep defaults conservative and suitable for "thinking" models.
    # Users can tune via env vars if needed.
    #
    # Note: On macOS Ollama often uses Metal (GPU) by default, so CPU usage may stay low.
    # These knobs mainly help saturate CPU for CPU-bound configs and/or improve throughput.
    def _f(name: str, default: float) -> float:
        raw = (os.environ.get(name) or "").strip()
        try:
            return float(raw) if raw else default
        except Exception:
            return default

    def _i(name: str, default: int) -> int:
        raw = (os.environ.get(name) or "").strip()
        try:
            return int(raw) if raw else default
        except Exception:
            return default

    def _ram_gb() -> Optional[float]:
        # Best-effort cross-platform physical RAM detection.
        try:
            page_size = os.sysconf("SC_PAGE_SIZE")
            pages = os.sysconf("SC_PHYS_PAGES")
            if page_size and pages:
                return (float(page_size) * float(pages)) / (1024**3)
        except Exception:
            return None
        return None

    perf = (os.environ.get("OLLAMA_PERF_PROFILE") or "auto").strip().lower()
    cpu_count = int(os.cpu_count() or 0)
    ram_gb = _ram_gb()

    options = {
        "temperature": _f("OLLAMA_TEMPERATURE", 0.7),
        "top_p": _f("OLLAMA_TOP_P", 0.95),
        "num_ctx": _i("OLLAMA_NUM_CTX", 8192),
    }

    # Explicit overrides (highest priority).
    raw_threads = (os.environ.get("OLLAMA_NUM_THREADS") or os.environ.get("OLLAMA_NUM_THREAD") or "").strip()
    raw_batch = (os.environ.get("OLLAMA_NUM_BATCH") or "").strip()
    if raw_threads.isdigit():
        options["num_thread"] = int(raw_threads)
    if raw_batch.isdigit():
        options["num_batch"] = int(raw_batch)

    # Auto-tuning for stronger machines.
    # Only apply when user didn't set explicit overrides.
    if "num_thread" not in options:
        if perf in {"max", "aggressive", "fast"}:
            if cpu_count:
                options["num_thread"] = max(4, cpu_count - 2)
        elif perf == "auto":
            if cpu_count >= 12 and (ram_gb is None or ram_gb >= 32):
                options["num_thread"] = max(4, cpu_count - 2)

    if "num_batch" not in options:
        if perf in {"max", "aggressive", "fast"}:
            # Higher batch can improve throughput on powerful machines.
            options["num_batch"] = 512
        elif perf == "auto":
            if cpu_count >= 12 and (ram_gb is None or ram_gb >= 32):
                options["num_batch"] = 256

    return options


_OLLAMA_PULL_LOCK = threading.Lock()


def _ensure_ollama_model_present(model: str) -> None:
    """
    Best-effort: ensure the model exists locally.
    If OLLAMA_AUTO_PULL=1, runs `ollama pull <model>` when missing.
    """
    base_url = _ollama_base_url()
    try:
        r = httpx.get(f"{base_url}/api/tags", timeout=10.0)
        r.raise_for_status()
        data = r.json() or {}
        models = data.get("models") or []
        if any((m or {}).get("name") == model for m in models):
            return
    except Exception:
        # If Ollama isn't reachable, don't try to pull.
        return

    if not _truthy_env("OLLAMA_AUTO_PULL", True):
        return

    with _OLLAMA_PULL_LOCK:
        # Double-check after waiting for the lock.
        try:
            r = httpx.get(f"{base_url}/api/tags", timeout=10.0)
            r.raise_for_status()
            data = r.json() or {}
            models = data.get("models") or []
            if any((m or {}).get("name") == model for m in models):
                return
        except Exception:
            return
        try:
            subprocess.run(["ollama", "pull", model], check=False)
        except Exception:
            pass


def _split_think_blocks(text: str) -> tuple[str, str]:
    """
    Splits a model output into (thoughts, final) using <think>...</think> blocks.
    Handles the common DeepSeek-R1 style.
    """
    s = text or ""
    thoughts = []
    final_parts = []
    idx = 0
    while True:
        start = s.find("<think>", idx)
        if start == -1:
            final_parts.append(s[idx:])
            break
        final_parts.append(s[idx:start])
        end = s.find("</think>", start + len("<think>"))
        if end == -1:
            thoughts.append(s[start + len("<think>") :])
            break
        thoughts.append(s[start + len("<think>") : end])
        idx = end + len("</think>")
    return ("".join(thoughts).strip(), "".join(final_parts).strip())


async def _ollama_stream_generate(
    *,
    model: str,
    prompt: str,
    system_prompt: Optional[str],
    on_update,
    cancel_event: Optional[asyncio.Event] = None,
) -> str:
    """
    Streams Ollama `/api/chat`. Calls `on_update(thoughts, elapsed_sec)` periodically.
    Returns final answer (tries to strip <think> blocks when present).
    """
    _ensure_ollama_model_present(model)
    url = f"{_ollama_base_url()}/api/chat"
    options = _ollama_options_from_env()
    try:
        logging.getLogger("local_telegram_bot").info(
            "Ollama generate (model=%s perf=%s options=%s)",
            model,
            (os.environ.get("OLLAMA_PERF_PROFILE") or "auto").strip().lower(),
            {k: options.get(k) for k in ("num_thread", "num_batch", "num_ctx", "temperature", "top_p") if k in options},
        )
    except Exception:
        pass
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt or ""},
            {"role": "user", "content": prompt},
        ],
        "stream": True,
        "options": options,
    }

    timeout = httpx.Timeout(connect=5.0, read=600.0, write=30.0, pool=30.0)
    raw = ""
    thoughts = ""
    started = time.monotonic()
    last_ui = 0.0
    ui_interval = 3.0

    async with httpx.AsyncClient(timeout=timeout) as client:
        async with client.stream("POST", url, json=payload) as r:
            r.raise_for_status()
            async for line in r.aiter_lines():
                if cancel_event is not None and cancel_event.is_set():
                    raise _UserCancelled()
                if not line:
                    continue
                try:
                    chunk = json.loads(line)
                except Exception:
                    continue
                msg = (chunk or {}).get("message") or {}
                raw += str(msg.get("content") or "")
                th, _final = _split_think_blocks(raw)
                thoughts = th

                now = time.monotonic()
                if (now - last_ui) >= ui_interval:
                    last_ui = now
                    try:
                        await on_update(thoughts, now - started)
                    except Exception:
                        pass

                if (chunk or {}).get("done") is True:
                    break

    try:
        await on_update(thoughts, time.monotonic() - started)
    except Exception:
        pass

    _thoughts, final = _split_think_blocks(raw)
    out = (final or raw).strip()
    if not out:
        raise RuntimeError("Local LLM –Ω–µ –≤–µ—Ä–Ω—É–ª —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç (–ø—É—Å—Ç–æ).")
    return out


async def handle_process_text(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    msg = update.effective_message
    if msg is None or not msg.text:
        return
    text = msg.text.strip()
    low = text.lower()

    # Email auth: if message looks like an email, treat it as an auth request
    # (works without /auth command).
    if _auth_enabled():
        m = re.match(r"^([^@\s]+@[^@\s]+\.[^@\s]+)$", text.strip().lower())
        if m:
            await _begin_email_auth(update, context, email=m.group(1))
            return

    # Email auth: if user has a pending code, try to verify it on any non-command text.
    user = update.effective_user
    if user is not None and _auth_enabled():
        user_id = int(user.id)
        if _auth_has_pending(user_id):
            ok, reply = _auth_verify_code(user_id, text)
            await msg.reply_text(reply)
            return
    return


def _is_group_chat(update: Update) -> bool:
    chat = update.effective_chat
    if not chat or not chat.type:
        return False
    try:
        return chat.type in {ChatType.GROUP, ChatType.SUPERGROUP}
    except Exception:
        # Fallback for older/newer PTB types.
        return str(chat.type).lower() in {"group", "supergroup"}


def _text_mentions_bot(text: str, *, bot_username: Optional[str]) -> bool:
    if not text or not bot_username:
        return False
    return f"@{bot_username.lower()}" in text.lower()


async def handle_group_tag(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """
    Group flow: reply to a voice/audio message and mention the bot (e.g. "@botname").
    """
    msg = update.effective_message
    if msg is None or not msg.text:
        return
    if not _is_group_chat(update):
        return

    replied = msg.reply_to_message
    if replied is None:
        return

    bot_username = context.application.bot_data.get("bot_username")
    if not bot_username:
        try:
            me = await context.bot.get_me()
            bot_username = me.username
            if bot_username:
                context.application.bot_data["bot_username"] = bot_username
        except Exception:
            bot_username = None
    if not _text_mentions_bot(msg.text, bot_username=bot_username):
        return

    normalize_audio = _should_normalize_audio(msg.text, bot_username=bot_username)

    try:
        ent_types = ",".join(sorted({(e.type or "") for e in (msg.entities or []) if e})) or "-"
        logging.getLogger("local_telegram_bot").info(
            "Group tag triggered chat=%s msg=%s reply_to=%s entities=%s normalize_audio=%s",
            getattr(update.effective_chat, "id", None),
            getattr(msg, "message_id", None),
            getattr(replied, "message_id", None),
            ent_types,
            "yes" if normalize_audio else "no",
        )
    except Exception:
        pass

    file_id, filename, file_size = _pick_telegram_file_from_message(replied)
    if not file_id:
        await msg.reply_text("–û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ voice/audio —Å–æ–æ–±—â–µ–Ω–∏–µ –∏ —É–ø–æ–º—è–Ω–∏—Ç–µ –º–µ–Ω—è (@...).")
        return

    await _process_audio(
        update,
        context,
        file_id=file_id,
        filename=filename or "audio",
        reply_target=msg,
        source_message_id=getattr(replied, "message_id", None),
        source_file_size=file_size,
        normalize_audio=normalize_audio,
    )


async def _process_audio(
    update: Update,
    context: ContextTypes.DEFAULT_TYPE,
    *,
    file_id: str,
    filename: str,
    reply_target,
    source_message_id: Optional[int] = None,
    source_file_size: Optional[int] = None,
    normalize_audio: bool = False,
) -> None:
    """
    Core pipeline used by both private chats (direct voice/audio)
    and group chats (reply+mention).
    """
    sem: asyncio.Semaphore = context.application.bot_data.setdefault(
        "asr_semaphore", asyncio.Semaphore(_asr_concurrency())
    )
    async with sem:
        session_id = uuid.uuid4().hex
        cancel_event = asyncio.Event()
        _active_sessions(context)[session_id] = {
            "cancel_event": cancel_event,
            "user_id": int(getattr(getattr(update, "effective_user", None), "id", 0) or 0),
            "chat_id": int(getattr(getattr(update, "effective_chat", None), "id", 0) or 0),
        }
        started_at = datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc).isoformat()
        t0 = time.monotonic()
        user = update.effective_user
        chat = update.effective_chat
        req_msg = update.effective_message
        session: dict = {
            "session_id": session_id,
            "started_at": started_at,
            "trigger": "group_mention" if _is_group_chat(update) else "private_audio",
            "telegram": {
                "user": {
                    "id": getattr(user, "id", None),
                    "username": getattr(user, "username", None),
                    "first_name": getattr(user, "first_name", None),
                    "last_name": getattr(user, "last_name", None),
                    "language_code": getattr(user, "language_code", None),
                },
                "chat": {
                    "id": getattr(chat, "id", None),
                    "type": str(getattr(chat, "type", "") or ""),
                    "title": getattr(chat, "title", None),
                },
                "request_message_id": getattr(reply_target, "message_id", None) or getattr(req_msg, "message_id", None),
                "audio_message_id": source_message_id
                or getattr(reply_target, "message_id", None)
                or getattr(req_msg, "message_id", None),
            },
            "audio": {
                "file_id": file_id,
                "filename": filename,
                "file_size": source_file_size,
                "normalize_audio": bool(normalize_audio),
            },
            "status": "started",
        }
        try:
            whisper_model = _get_env("WHISPER_MODEL", "medium")
            gigaam_model = _get_env("GIGAAM_MODEL", "v3_e2e_rnnt")
            device = _get_env("DEVICE", "cpu")
            language = _get_env("LANGUAGE", "ru")
            hf_token: Optional[str] = (os.environ.get("HF_TOKEN") or "").strip() or None
            gemini_api_key = (os.environ.get("GEMINI_API_KEY") or "").strip()
            gemini_model = (os.environ.get("GEMINI_MODEL") or "gemini-3-pro-preview").strip()
            gemini_system_prompt, prompt_src = _get_system_prompt()
            session["models"] = {
                "whisper": whisper_model,
                "gigaam": gigaam_model,
                "gemini": gemini_model if gemini_api_key else None,
                "device": device,
                "language": language,
                "hf_token": bool(hf_token),
                "llm_system_prompt": prompt_src,
            }

            # Authorization (optional) / whitelist:
            # When AUTH_ENABLED=1, Gemini is allowed only for authorized users,
            # but an authorized user can "whitelist" a group chat on first use.
            user_id0 = int(getattr(user, "id", 0) or 0)
            chat_id0 = int(getattr(chat, "id", 0) or 0)
            auth_enabled = _auth_enabled()
            user_authorized = bool(user_id0 and _auth_is_user_authorized(user_id0))
            chat_whitelisted = bool(chat_id0 and _auth_is_chat_whitelisted(chat_id0))
            whitelisted_now = False
            if auth_enabled and user_id0 and chat_id0 and _is_group_chat(update):
                whitelisted_now = _auth_maybe_whitelist_chat(user_id=user_id0, chat_id=chat_id0)
                if whitelisted_now:
                    chat_whitelisted = True
            can_use_gemini = bool(user_id0 and chat_id0 and ((not auth_enabled) or user_authorized or chat_whitelisted))
            session["auth"] = {
                "enabled": auth_enabled,
                "user_authorized": user_authorized,
                "chat_whitelisted": chat_whitelisted,
                "chat_whitelisted_now": whitelisted_now,
                "can_use_gemini": can_use_gemini,
            }

            if user_id0 and not can_use_gemini and _auth_should_prompt_user(user_id0):
                # Don't block: just inform once how to enable Gemini.
                _auth_mark_prompted(user_id0)
                domain = _auth_domain()
                await reply_target.reply_text(
                    f"‚ÑπÔ∏è Gemini –¥–æ—Å—Ç—É–ø–µ–Ω —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –ø–æ –¥–æ–º–µ–Ω—É {domain}.\n"
                    f"–ß—Ç–æ–±—ã –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞—Ç—å—Å—è, –æ—Ç–ø—Ä–∞–≤—å—Ç–µ –º–Ω–µ (–≤ –ª–∏—á–∫—É) –≤–∞—à—É –ø–æ—á—Ç—É –≤–∏–¥–∞ `email@{domain}`."
                )

            def _check_cancel() -> None:
                if cancel_event.is_set():
                    raise _UserCancelled()

            markup = _cancel_markup(session_id)

            # Single "status/progress" message. It is recreated between stages so it's always below previously sent text.
            progress = await reply_target.reply_text("–°—Ç–∞—Ä—Ç‚Ä¶", reply_markup=markup)
            chat_id = int(update.effective_chat.id) if update.effective_chat else None

            await _safe_edit_message(progress, "üì• –°–∫–∞—á–∏–≤–∞—é –∞—É–¥–∏–æ‚Ä¶", reply_markup=markup)
            max_bytes = _telegram_max_get_file_bytes()
            if max_bytes and source_file_size and source_file_size > max_bytes:
                size_mb = source_file_size / (1024 * 1024)
                limit_mb = max_bytes / (1024 * 1024)
                session["status"] = "error"
                session["error"] = f"incoming_file_too_big({size_mb:.1f}MB>{limit_mb:.0f}MB)"
                await _safe_delete(progress)
                await reply_target.reply_text(_file_too_big_message(size_mb=size_mb, limit_mb=limit_mb))
                return

            try:
                tg_file = await context.bot.get_file(file_id)
            except BadRequest as exc:
                if "file is too big" in str(exc).lower():
                    size_mb = (source_file_size or 0) / (1024 * 1024)
                    max_bytes = _telegram_max_get_file_bytes()
                    limit_mb = (max_bytes / (1024 * 1024)) if max_bytes else 20
                    session["status"] = "error"
                    session["error"] = "telegram_get_file_too_big"
                    await _safe_delete(progress)
                    await reply_target.reply_text(_file_too_big_message(size_mb=size_mb, limit_mb=limit_mb))
                    return
                raise

            with tempfile.TemporaryDirectory(prefix="tg_asr_") as td:
                _check_cancel()
                src_path = os.path.join(td, filename)
                wav_dir = td

                dl0 = time.monotonic()
                await tg_file.download_to_drive(custom_path=src_path)
                _check_cancel()
                session["timings"] = {"download_sec": round(time.monotonic() - dl0, 3)}

                ap = AudioProcessor()
                loop = asyncio.get_running_loop()

                if normalize_audio:
                    await _safe_edit_message(
                        progress,
                        "‚úÖ –ê—É–¥–∏–æ —Å–∫–∞—á–∞–Ω–æ\nüéö –ù–æ—Ä–º–∞–ª–∏–∑—É—é –∞—É–¥–∏–æ –∏ –≥–æ—Ç–æ–≤–ª—é WAV (16kHz mono)‚Ä¶",
                        reply_markup=markup,
                    )
                else:
                    await _safe_edit_message(progress, "‚úÖ –ê—É–¥–∏–æ —Å–∫–∞—á–∞–Ω–æ\nüéõ –ì–æ—Ç–æ–≤–ª—é WAV (16kHz mono)‚Ä¶", reply_markup=markup)
                ex0 = time.monotonic()
                audio_filter = _audio_normalize_filter() if normalize_audio else None
                if normalize_audio:
                    try:
                        logging.getLogger("local_telegram_bot").info("Audio normalize enabled (filter=%s)", audio_filter)
                    except Exception:
                        pass
                wav_path = await loop.run_in_executor(
                    None, lambda: ap.extract_audio(src_path, output_dir=wav_dir, audio_filter=audio_filter)
                )
                _check_cancel()
                session["audio"]["wav_path"] = os.path.basename(wav_path)
                session["audio"]["wav_sec"] = round(_wav_duration_sec(wav_path), 3)
                session["timings"]["extract_wav_sec"] = round(time.monotonic() - ex0, 3)

                if normalize_audio:
                    await _safe_edit_message(
                        progress,
                        "‚úÖ WAV –≥–æ—Ç–æ–≤\n"
                        "üéß –û—Ç–ø—Ä–∞–≤–ª—è—é –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è + —á–∏—Å—Ç–∫–∞ —â–µ–ª—á–∫–æ–≤)‚Ä¶",
                        reply_markup=markup,
                    )
                    _check_cancel()
                    try:
                        processed_path = await loop.run_in_executor(
                            None,
                            lambda: _encode_audio_preview_ffmpeg(
                                ffmpeg_bin=getattr(ap, "ffmpeg_bin", "ffmpeg"),
                                src_wav=wav_path,
                                out_dir=wav_dir,
                                base_name=f"processed_{session_id[:8]}",
                            ),
                        )
                        session["audio"]["processed_audio_path"] = os.path.basename(processed_path)
                        session["audio"]["processed_audio_bytes"] = int(os.path.getsize(processed_path))
                        try:
                            logging.getLogger("local_telegram_bot").info(
                                "Sending processed audio: %s (%d bytes)", processed_path, session["audio"]["processed_audio_bytes"]
                            )
                        except Exception:
                            pass
                        with open(processed_path, "rb") as f:
                            try:
                                await reply_target.reply_audio(
                                    audio=f,
                                    filename=os.path.basename(processed_path),
                                    caption="üéß –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è + —á–∏—Å—Ç–∫–∞ —â–µ–ª—á–∫–æ–≤)",
                                )
                            except Exception:
                                await reply_target.reply_document(
                                    document=f,
                                    filename=os.path.basename(processed_path),
                                    caption="üéß –û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ (–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è + —á–∏—Å—Ç–∫–∞ —â–µ–ª—á–∫–æ–≤)",
                                )
                    except Exception as exc:
                        logging.getLogger("local_telegram_bot").exception("Failed to send processed audio")
                        await reply_target.reply_text(f"‚ö†Ô∏è –ù–µ —Å–º–æ–≥ –æ—Ç–ø—Ä–∞–≤–∏—Ç—å –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω–æ–µ –∞—É–¥–∏–æ: {str(exc).strip()}")

                    # Recreate progress message so it stays below the sent audio.
                    await _safe_delete(progress)
                    progress = await reply_target.reply_text(
                        f"üß† Whisper ({whisper_model}) ‚Äî —Ä–∞—Å–ø–æ–∑–Ω–∞—é‚Ä¶\n"
                        "(–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –º–æ–∂–µ—Ç –∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å; –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ beam_size=1)",
                        reply_markup=markup,
                    )
                else:
                    await _safe_edit_message(
                        progress,
                        "‚úÖ WAV –≥–æ—Ç–æ–≤\n"
                        f"üß† Whisper ({whisper_model}) ‚Äî —Ä–∞—Å–ø–æ–∑–Ω–∞—é‚Ä¶\n"
                        "(–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –º–æ–∂–µ—Ç –∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å; –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ beam_size=1)",
                        reply_markup=markup,
                    )
                cpu_threads = int((os.environ.get("WHISPER_CPU_THREADS") or "").strip() or "0")
                whisper = TranscriptionService(
                    model_size=whisper_model,
                    device=device,
                    language=language,
                    cpu_threads=cpu_threads,
                )
                ticker_task = None
                whisper_state: Optional[_ProgressState] = None
                if chat_id is not None:
                    audio_sec = _wav_duration_sec(wav_path)
                    whisper_state = _ProgressState(audio_sec=audio_sec, est_rtf=_get_rtf_est("whisper"))
                    ticker_task = asyncio.create_task(
                        _ticker(
                            context=context,
                            chat_id=chat_id,
                            message=progress,
                            base_text=f"üß† Whisper ({whisper_model}) ‚Äî —Ä–∞—Å–ø–æ–∑–Ω–∞—é‚Ä¶",
                            reply_markup=markup,
                            state=whisper_state,
                            cancel_event=cancel_event,
                        )
                    )
                try:
                    audio_sec0 = float(getattr(whisper_state, "audio_sec", 0.0) or 0.0) if whisper_state else 0.0
                    whisper_timeout_env = _parse_timeout_env("WHISPER_TIMEOUT_SEC")
                    if whisper_timeout_env is None:
                        whisper_timeout = _default_whisper_timeout_sec(audio_sec=audio_sec0)
                    else:
                        whisper_timeout = float(whisper_timeout_env)
                    w0 = time.monotonic()
                    cancel_thread_event = threading.Event()

                    async def _mirror_cancel_to_thread() -> None:
                        try:
                            await cancel_event.wait()
                            cancel_thread_event.set()
                        except Exception:
                            pass

                    mirror_task = asyncio.create_task(_mirror_cancel_to_thread())
                    try:
                        if whisper_timeout <= 0:
                            w_segments, _w_info = await loop.run_in_executor(
                                None,
                                lambda: whisper.transcribe(
                                    wav_path,
                                    progress_cb=(whisper_state.set_processed_sec if whisper_state else None),
                                    cancel_cb=cancel_thread_event.is_set,
                                ),
                            )
                        else:
                            w_segments, _w_info = await asyncio.wait_for(
                                loop.run_in_executor(
                                    None,
                                    lambda: whisper.transcribe(
                                        wav_path,
                                        progress_cb=(whisper_state.set_processed_sec if whisper_state else None),
                                        cancel_cb=cancel_thread_event.is_set,
                                    ),
                                ),
                                timeout=float(max(10.0, whisper_timeout)),
                            )
                    except TranscriptionCancelled:
                        raise _UserCancelled()
                    except asyncio.TimeoutError as exc:
                        w_wall = time.monotonic() - w0
                        session["timings"]["whisper_sec"] = round(w_wall, 3)
                        raise RuntimeError(
                            "Whisper –Ω–µ —É—Å–ø–µ–ª –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∏ –±—ã–ª –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ —Ç–∞–π–º–∞—É—Ç—É.\n"
                            f"–í—Ä–µ–º—è: {_fmt_dur(w_wall)} / –ª–∏–º–∏—Ç: {_fmt_dur(float(whisper_timeout))}.\n"
                            "–£–≤–µ–ª–∏—á—å—Ç–µ `WHISPER_TIMEOUT_SEC` (–Ω–∞–ø—Ä–∏–º–µ—Ä, 3600) –∏–ª–∏ –æ—Ç–∫–ª—é—á–∏—Ç–µ —Ç–∞–π–º–∞—É—Ç: `WHISPER_TIMEOUT_SEC=0`."
                        ) from exc
                    finally:
                        try:
                            mirror_task.cancel()
                        except Exception:
                            pass
                    w_wall = time.monotonic() - w0
                    if whisper_state and whisper_state.audio_sec > 0:
                        _update_rtf_est("whisper", w_wall / whisper_state.audio_sec)
                    session["timings"]["whisper_sec"] = round(w_wall, 3)
                finally:
                    if ticker_task:
                        ticker_task.cancel()
                        try:
                            await ticker_task
                        except Exception:
                            pass
                _check_cancel()
                w_text = " ".join((s.text or "").strip() for s in w_segments if (s.text or "").strip()).strip()
                session["results"] = {
                    "whisper_len": len(w_text),
                    "whisper_segments": len(w_segments or []),
                }
                await _safe_edit_message(
                    progress,
                    "‚úÖ Whisper –≥–æ—Ç–æ–≤\n"
                    f"üß† GigaAM ({gigaam_model}) ‚Äî —Ä–∞—Å–ø–æ–∑–Ω–∞—é‚Ä¶\n"
                    "(–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –º–æ–∂–µ—Ç –∫–∞—á–∞—Ç—å –≤–µ—Å–∞)",
                    reply_markup=markup,
                )

                giga = GigaAMTranscriptionService(model_name=gigaam_model, device=device, hf_token=hf_token)
                ticker_task = None
                giga_state: Optional[_ProgressState] = None
                if chat_id is not None:
                    audio_sec = _wav_duration_sec(wav_path)
                    giga_state = _ProgressState(audio_sec=audio_sec, est_rtf=_get_rtf_est("gigaam"))
                    ticker_task = asyncio.create_task(
                        _ticker(
                            context=context,
                            chat_id=chat_id,
                            message=progress,
                            base_text=f"üß† GigaAM ({gigaam_model}) ‚Äî —Ä–∞—Å–ø–æ–∑–Ω–∞—é‚Ä¶",
                            reply_markup=markup,
                            state=giga_state,
                            cancel_event=cancel_event,
                        )
                    )
                try:
                    g0 = time.monotonic()
                    hard_cancel = _truthy_env("GIGAAM_HARD_CANCEL", True)
                    if hard_cancel:
                        try:
                            logging.getLogger("local_telegram_bot").info("GigaAM hard-cancel: enabled (subprocess)")
                        except Exception:
                            pass
                        seg_tuples, _g_info = await _gigaam_transcribe_hard_cancel(
                            wav_path=wav_path,
                            model_name=gigaam_model,
                            device=device,
                            hf_token=hf_token,
                            cancel_event=cancel_event,
                        )
                        g_segments = [ASRSegment(start=a, end=b, text=t) for (a, b, t) in (seg_tuples or [])]
                    else:
                        g_segments, _g_info = await loop.run_in_executor(None, lambda: giga.transcribe(wav_path))
                    g_wall = time.monotonic() - g0
                    if giga_state and giga_state.audio_sec > 0:
                        _update_rtf_est("gigaam", g_wall / giga_state.audio_sec)
                    session["timings"]["gigaam_sec"] = round(g_wall, 3)
                    if isinstance(_g_info, dict):
                        session.setdefault("gigaam", {})["info"] = _g_info
                finally:
                    if ticker_task:
                        ticker_task.cancel()
                        try:
                            await ticker_task
                        except Exception:
                            pass
                _check_cancel()
                g_text = " ".join((s.text or "").strip() for s in g_segments if (s.text or "").strip()).strip()
                session["results"].update(
                    {
                        "gigaam_len": len(g_text),
                        "gigaam_segments": len(g_segments or []),
                    }
                )

                transcripts_exceed_single_message = (
                    len(w_text) > TELEGRAM_TEXT_LIMIT or len(g_text) > TELEGRAM_TEXT_LIMIT
                )
                session["results"]["transcripts_exceed_single_message"] = transcripts_exceed_single_message
                if transcripts_exceed_single_message:
                    await _safe_edit_message(
                        progress,
                        "‚ÑπÔ∏è –¢–µ–∫—Å—Ç —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π –¥–ª—è –æ–¥–Ω–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è.\n"
                        "–ò—Ç–æ–≥ –∏ –æ–±–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –ø—Ä–∏—à–ª—é —Ç–æ–ª—å–∫–æ —Ñ–∞–π–ª–æ–º‚Ä¶",
                        reply_markup=markup,
                    )
                else:
                    await _safe_delete(progress)
                    progress = None
                    await _reply_long(update, f"Whisper:\n{w_text or '(–ø—É—Å—Ç–æ)'}")
                    await _reply_long(update, f"GigaAM:\n{g_text or '(–ø—É—Å—Ç–æ)'}")

                final_text = ""
                final_error = ""
                final_label = ""

                user_prompt = (
                    "Whisper:\n"
                    f"{w_text.strip()}\n\n"
                    "GigaAM:\n"
                    f"{g_text.strip()}\n"
                )

                if chat_id is not None:
                    await _safe_delete(progress)
                    _check_cancel()

                    llm_sem: asyncio.Semaphore = context.application.bot_data.setdefault(
                        "llm_semaphore", asyncio.Semaphore(_llm_concurrency())
                    )

                    if can_use_gemini and gemini_api_key:
                        final_label = f"Gemini ({gemini_model})"
                        progress = await reply_target.reply_text(
                            f"üß† {final_label} ‚Äî –∂–¥—É –æ—á–µ—Ä–µ–¥—å‚Ä¶", reply_markup=markup
                        )
                        gmi0 = time.monotonic()

                        async def on_update(thoughts: str, elapsed_sec: float) -> None:
                            body = _trim_for_telegram(thoughts or "(–ø–æ–∫–∞ –±–µ–∑ –º—ã—Å–ª–µ–π)")
                            body_html = _markdown_bold_lines_to_html(body)
                            summary = _active_sessions_summary_line(context)
                            summary_line = f"\n{_html.escape(summary)}" if summary else ""
                            text_html = (
                                f"üß† { _html.escape(final_label) } ‚Äî –¥—É–º–∞—é –Ω–∞–¥ –∏—Ç–æ–≥–æ–º‚Ä¶\n"
                                f"‚è± {_html.escape(_fmt_dur(elapsed_sec))}{summary_line}\n\n"
                                f"{body_html}"
                            )
                            await _safe_edit_message(progress, text_html, parse_mode=ParseMode.HTML, reply_markup=markup)
                            try:
                                await context.bot.send_chat_action(chat_id=int(chat_id), action=ChatAction.TYPING)
                            except Exception:
                                pass

                        generation_config = {
                            "temperature": _parse_float_env(os.environ.get("GEMINI_TEMPERATURE") or "1", 1.0),
                            "topP": _parse_float_env(os.environ.get("GEMINI_TOP_P") or "0.95", 0.95),
                            "maxOutputTokens": _parse_int_env(
                                os.environ.get("GEMINI_MAX_OUTPUT_TOKENS") or "65536", 65536
                            ),
                            "mediaResolution": _media_resolution_value(os.environ.get("GEMINI_MEDIA_RESOLUTION") or "default"),
                            "thinkingConfig": {
                                "thinkingLevel": _thinking_level_value(os.environ.get("GEMINI_THINKING_LEVEL") or "high"),
                                "includeThoughts": True,
                            },
                        }

                        try:
                            async with llm_sem:
                                await _safe_edit_message(
                                    progress, f"üß† {final_label} ‚Äî –¥—É–º–∞—é –Ω–∞–¥ –∏—Ç–æ–≥–æ–º‚Ä¶", reply_markup=markup
                                )
                                final_text = await _gemini_stream_generate(
                                    api_key=gemini_api_key,
                                    model=gemini_model,
                                    prompt=user_prompt,
                                    system_prompt=gemini_system_prompt,
                                    generation_config=generation_config,
                                    on_update=on_update,
                                    cancel_event=cancel_event,
                                )
                        except Exception as exc:
                            logging.getLogger("local_telegram_bot").exception("Gemini default processing failed")
                            final_error = str(exc)
                        finally:
                            session["timings"]["llm_sec"] = round(time.monotonic() - gmi0, 3)
                            session["results"]["llm_provider"] = "gemini"
                            session["results"]["llm_model"] = gemini_model
                    else:
                        local_model = _ollama_model()
                        final_label = f"Local ({local_model})"
                        progress = await reply_target.reply_text(
                            f"üß† {final_label} ‚Äî –∂–¥—É –æ—á–µ—Ä–µ–¥—å‚Ä¶", reply_markup=markup
                        )
                        llm0 = time.monotonic()

                        async def on_update(thoughts: str, elapsed_sec: float) -> None:
                            body = _trim_for_telegram(thoughts or "(–¥—É–º–∞—é‚Ä¶)")
                            body_html = _markdown_bold_lines_to_html(body)
                            summary = _active_sessions_summary_line(context)
                            summary_line = f"\n{_html.escape(summary)}" if summary else ""
                            text_html = (
                                f"üß† { _html.escape(final_label) } ‚Äî –¥—É–º–∞—é –Ω–∞–¥ –∏—Ç–æ–≥–æ–º‚Ä¶\n"
                                f"‚è± {_html.escape(_fmt_dur(elapsed_sec))}{summary_line}\n\n"
                                f"{body_html}"
                            )
                            await _safe_edit_message(progress, text_html, parse_mode=ParseMode.HTML, reply_markup=markup)
                            try:
                                await context.bot.send_chat_action(chat_id=int(chat_id), action=ChatAction.TYPING)
                            except Exception:
                                pass

                        try:
                            async with llm_sem:
                                await _safe_edit_message(
                                    progress, f"üß† {final_label} ‚Äî –¥—É–º–∞—é –Ω–∞–¥ –∏—Ç–æ–≥–æ–º‚Ä¶", reply_markup=markup
                                )
                                final_text = await _ollama_stream_generate(
                                    model=local_model,
                                    prompt=user_prompt,
                                    system_prompt=gemini_system_prompt,
                                    on_update=on_update,
                                    cancel_event=cancel_event,
                                )
                        except Exception as exc:
                            logging.getLogger("local_telegram_bot").exception("Local LLM processing failed")
                            final_error = str(exc)
                        finally:
                            session["timings"]["llm_sec"] = round(time.monotonic() - llm0, 3)
                            session["results"]["llm_provider"] = "ollama"
                            session["results"]["llm_model"] = local_model
                else:
                    final_error = "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å chat_id –¥–ª—è LLM."
                    final_label = "LLM"

                if progress:
                    await _safe_edit_message(progress, "üìÑ –§–æ—Ä–º–∏—Ä—É—é –∏—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª‚Ä¶", reply_markup=markup)

                if final_text and not transcripts_exceed_single_message:
                    chat_text = _gemini_chat_excerpt(final_text)
                    if chat_text.strip():
                        await _reply_long_html(update, _markdown_to_telegram_html(chat_text.strip()))
                elif final_error:
                    await reply_target.reply_text(f"LLM: –æ—à–∏–±–∫–∞/–ø—Ä–æ–ø—É—Å–∫: {final_error}")

                session["results"].update(
                    {
                        "llm_used": bool(final_text),
                        "llm_error": final_error or None,
                        "llm_len": len(final_text or ""),
                    }
                )

                await _send_markdown_file(
                    update,
                    final_text=final_text,
                    final_error=final_error,
                    final_label=final_label or "LLM",
                    whisper_text=w_text,
                    gigaam_text=g_text,
                )
                session.setdefault("telegram", {}).update(
                    {
                        "sent_transcripts_to_chat": not transcripts_exceed_single_message,
                        "sent_gemini_to_chat": bool(final_text) and not transcripts_exceed_single_message,
                        "sent_markdown_file": True,
                    }
                )

                await _safe_delete(progress)
                await reply_target.reply_text("‚úÖ –ì–æ—Ç–æ–≤–æ")
                session["status"] = "ok"
        except _UserCancelled:
            session["status"] = "canceled"
            session["error"] = "canceled_by_user"
            try:
                await _safe_edit_message(progress, "‚õîÔ∏è –û—Ç–º–µ–Ω–µ–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.", reply_markup=None)
            except Exception:
                pass
        except Exception as exc:
            logging.exception("ASR failed")
            err = str(exc).strip() or exc.__class__.__name__
            await reply_target.reply_text(f"–û—à–∏–±–∫–∞: {err}")
            session["status"] = "error"
            session["error"] = err
        finally:
            try:
                _active_sessions(context).pop(session_id, None)
            except Exception:
                pass
            session["ended_at"] = datetime.datetime.utcnow().replace(tzinfo=datetime.timezone.utc).isoformat()
            session["timings"] = session.get("timings") or {}
            session["timings"]["total_sec"] = round(time.monotonic() - t0, 3)
            _log_usage_session(session)

async def handle_audio(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    # In groups, process only via reply+mention flow to avoid accidental triggers/spam.
    if _is_group_chat(update):
        return

    file_id, filename, file_size = _pick_telegram_file(update)
    if not file_id:
        await update.effective_message.reply_text("–ü—Ä–∏—à–ª–∏—Ç–µ voice/audio —Ñ–∞–π–ª.")
        return

    # Use the shared pipeline (also used by reply+mention in group chats).
    await _process_audio(
        update,
        context,
        file_id=file_id,
        filename=filename or "audio",
        reply_target=update.effective_message,
        source_message_id=getattr(update.effective_message, "message_id", None),
        source_file_size=file_size,
    )
    return

    sem: asyncio.Semaphore = context.application.bot_data.setdefault(
        "asr_semaphore", asyncio.Semaphore(_asr_concurrency())
    )
    async with sem:
        try:
            whisper_model = _get_env("WHISPER_MODEL", "medium")
            gigaam_model = _get_env("GIGAAM_MODEL", "v3_e2e_rnnt")
            device = _get_env("DEVICE", "cpu")
            language = _get_env("LANGUAGE", "ru")
            hf_token: Optional[str] = (os.environ.get("HF_TOKEN") or "").strip() or None
            gemini_api_key = (os.environ.get("GEMINI_API_KEY") or "").strip()
            gemini_model = (os.environ.get("GEMINI_MODEL") or "gemini-3-pro-preview").strip()
            gemini_system_prompt, _prompt_src = _get_system_prompt()

            # Single "status/progress" message. It is recreated between stages so it's always below previously sent text.
            progress = await update.effective_message.reply_text("–°—Ç–∞—Ä—Ç‚Ä¶")
            chat_id = int(update.effective_chat.id) if update.effective_chat else None

            await _safe_edit(progress, "üì• –°–∫–∞—á–∏–≤–∞—é –∞—É–¥–∏–æ‚Ä¶")
            tg_file = await context.bot.get_file(file_id)

            with tempfile.TemporaryDirectory(prefix="tg_asr_") as td:
                src_path = os.path.join(td, filename)
                wav_dir = td

                await tg_file.download_to_drive(custom_path=src_path)

                ap = AudioProcessor()
                loop = asyncio.get_running_loop()

                await _safe_edit(progress, "‚úÖ –ê—É–¥–∏–æ —Å–∫–∞—á–∞–Ω–æ\nüéõ –ì–æ—Ç–æ–≤–ª—é WAV (16kHz mono)‚Ä¶")
                wav_path = await loop.run_in_executor(None, lambda: ap.extract_audio(src_path, output_dir=wav_dir))

                await _safe_edit(
                    progress,
                    "‚úÖ WAV –≥–æ—Ç–æ–≤\n"
                    f"üß† Whisper ({whisper_model}) ‚Äî —Ä–∞—Å–ø–æ–∑–Ω–∞—é‚Ä¶\n"
                    "(–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –º–æ–∂–µ—Ç –∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å; –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ beam_size=1)",
                )
                cpu_threads = int((os.environ.get("WHISPER_CPU_THREADS") or "").strip() or "0")
                whisper = TranscriptionService(
                    model_size=whisper_model,
                    device=device,
                    language=language,
                    cpu_threads=cpu_threads,
                )
                ticker_task = None
                whisper_state: Optional[_ProgressState] = None
                if chat_id is not None:
                    audio_sec = _wav_duration_sec(wav_path)
                    whisper_state = _ProgressState(audio_sec=audio_sec, est_rtf=_get_rtf_est("whisper"))
                    ticker_task = asyncio.create_task(
                        _ticker(
                            context=context,
                            chat_id=chat_id,
                            message=progress,
                            base_text=f"üß† Whisper ({whisper_model}) ‚Äî —Ä–∞—Å–ø–æ–∑–Ω–∞—é‚Ä¶",
                            state=whisper_state,
                        )
                    )
                try:
                    whisper_timeout = int((os.environ.get("WHISPER_TIMEOUT_SEC") or "").strip() or "240")
                    w0 = time.monotonic()
                    w_segments, _w_info = await asyncio.wait_for(
                        loop.run_in_executor(
                            None,
                            lambda: whisper.transcribe(
                                wav_path,
                                progress_cb=(whisper_state.set_processed_sec if whisper_state else None),
                            ),
                        ),
                        timeout=float(max(10, whisper_timeout)),
                    )
                    w_wall = time.monotonic() - w0
                    if whisper_state and whisper_state.audio_sec > 0:
                        _update_rtf_est("whisper", w_wall / whisper_state.audio_sec)
                finally:
                    if ticker_task:
                        ticker_task.cancel()
                        try:
                            await ticker_task
                        except Exception:
                            pass
                w_text = " ".join((s.text or "").strip() for s in w_segments if (s.text or "").strip()).strip()
                await _reply_long(update, f"Whisper:\n{w_text or '(–ø—É—Å—Ç–æ)'}")

                # Recreate status message so it stays below the Whisper output.
                await _safe_delete(progress)
                progress = await update.effective_message.reply_text(
                    f"üß† GigaAM ({gigaam_model}) ‚Äî —Ä–∞—Å–ø–æ–∑–Ω–∞—é‚Ä¶\n(–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –º–æ–∂–µ—Ç –∫–∞—á–∞—Ç—å –≤–µ—Å–∞)"
                )

                await _safe_edit(
                    progress,
                    f"üß† GigaAM ({gigaam_model}) ‚Äî —Ä–∞—Å–ø–æ–∑–Ω–∞—é‚Ä¶\n(–ø–µ—Ä–≤—ã–π –∑–∞–ø—É—Å–∫ –º–æ–∂–µ—Ç –∫–∞—á–∞—Ç—å –≤–µ—Å–∞)",
                )
                giga = GigaAMTranscriptionService(model_name=gigaam_model, device=device, hf_token=hf_token)
                ticker_task = None
                giga_state: Optional[_ProgressState] = None
                if chat_id is not None:
                    audio_sec = _wav_duration_sec(wav_path)
                    giga_state = _ProgressState(audio_sec=audio_sec, est_rtf=_get_rtf_est("gigaam"))
                    ticker_task = asyncio.create_task(
                        _ticker(
                            context=context,
                            chat_id=chat_id,
                            message=progress,
                            base_text=f"üß† GigaAM ({gigaam_model}) ‚Äî —Ä–∞—Å–ø–æ–∑–Ω–∞—é‚Ä¶",
                            state=giga_state,
                        )
                    )
                try:
                    g0 = time.monotonic()
                    g_segments, _g_info = await loop.run_in_executor(None, lambda: giga.transcribe(wav_path))
                    g_wall = time.monotonic() - g0
                    if giga_state and giga_state.audio_sec > 0:
                        _update_rtf_est("gigaam", g_wall / giga_state.audio_sec)
                finally:
                    if ticker_task:
                        ticker_task.cancel()
                        try:
                            await ticker_task
                        except Exception:
                            pass
                g_text = " ".join((s.text or "").strip() for s in g_segments if (s.text or "").strip()).strip()
                await _reply_long(update, f"GigaAM:\n{g_text or '(–ø—É—Å—Ç–æ)'}")

                # Gemini post-processing (default pipeline) ‚Üí markdown file.
                gemini_text = ""
                gemini_error = ""
                if gemini_api_key and chat_id is not None:
                    # Recreate status message so Gemini thoughts are always below the GigaAM output.
                    await _safe_delete(progress)
                    progress = await update.effective_message.reply_text(
                        f"üß† Gemini ({gemini_model}) ‚Äî –¥—É–º–∞—é –Ω–∞–¥ –∏—Ç–æ–≥–æ–º‚Ä¶"
                    )

                    async def on_update(thoughts: str, elapsed_sec: float) -> None:
                        body = _trim_for_telegram(thoughts or "(–ø–æ–∫–∞ –±–µ–∑ –º—ã—Å–ª–µ–π)")
                        body_html = _markdown_bold_lines_to_html(body)
                        text_html = (
                            f"üß† Gemini ({_html.escape(gemini_model)}) ‚Äî –¥—É–º–∞—é –Ω–∞–¥ –∏—Ç–æ–≥–æ–º‚Ä¶\n"
                            f"‚è± {_html.escape(_fmt_dur(elapsed_sec))}\n\n"
                            f"{body_html}"
                        )
                        await _safe_edit_formatted(progress, text_html, parse_mode=ParseMode.HTML)
                        try:
                            await context.bot.send_chat_action(chat_id=int(chat_id), action=ChatAction.TYPING)
                        except Exception:
                            pass

                    system_prompt = gemini_system_prompt
                    user_prompt = (
                        "Whisper:\n"
                        f"{w_text.strip()}\n\n"
                        "GigaAM:\n"
                        f"{g_text.strip()}\n"
                    )
                    generation_config = {
                        "temperature": _parse_float_env(os.environ.get("GEMINI_TEMPERATURE") or "1", 1.0),
                        "topP": _parse_float_env(os.environ.get("GEMINI_TOP_P") or "0.95", 0.95),
                        "maxOutputTokens": _parse_int_env(os.environ.get("GEMINI_MAX_OUTPUT_TOKENS") or "65536", 65536),
                        "mediaResolution": _media_resolution_value(os.environ.get("GEMINI_MEDIA_RESOLUTION") or "default"),
                        "thinkingConfig": {
                            "thinkingLevel": _thinking_level_value(os.environ.get("GEMINI_THINKING_LEVEL") or "high"),
                            "includeThoughts": True,
                        },
                    }
                    try:
                        gemini_text = await _gemini_stream_generate(
                            api_key=gemini_api_key,
                            model=gemini_model,
                            prompt=user_prompt,
                            system_prompt=system_prompt,
                            generation_config=generation_config,
                            on_update=on_update,
                        )
                    except Exception as exc:
                        logging.getLogger("local_telegram_bot").exception("Gemini default processing failed")
                        gemini_error = str(exc)
                elif not gemini_api_key:
                    gemini_error = "GEMINI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é –æ–±—Ä–∞–±–æ—Ç–∫—É Gemini."

                # Send markdown file in requested order.
                if progress:
                    await _safe_edit(progress, "üìÑ –§–æ—Ä–º–∏—Ä—É—é –∏—Ç–æ–≥–æ–≤—ã–π —Ñ–∞–π–ª‚Ä¶")

                # Also send the final Gemini block as a chat message (then the file).
                if gemini_text:
                    chat_text = _gemini_chat_excerpt(gemini_text)
                    if chat_text.strip():
                        await _reply_long_html(update, _markdown_to_telegram_html(chat_text.strip()))
                elif gemini_error:
                    await update.effective_message.reply_text(f"Gemini: –æ—à–∏–±–∫–∞/–ø—Ä–æ–ø—É—Å–∫: {gemini_error}")

                await _send_markdown_file(
                    update,
                    final_text=gemini_text,
                    final_error=gemini_error,
                    final_label=f"Gemini ({gemini_model})",
                    whisper_text=w_text,
                    gigaam_text=g_text,
                )

                await _safe_delete(progress)
                await update.effective_message.reply_text("‚úÖ –ì–æ—Ç–æ–≤–æ")

        except Exception as exc:
            logging.exception("ASR failed")
            await update.effective_message.reply_text(f"–û—à–∏–±–∫–∞: {exc}")


def main() -> None:
    load_dotenv()
    # Keep `bot.log` readable: disable third-party progress bars in console.
    os.environ.setdefault("HF_HUB_DISABLE_PROGRESS_BARS", "1")
    os.environ.setdefault("TQDM_DISABLE", "1")
    # Reduce noisy third-party warnings in `bot.log`.
    try:
        from urllib3.exceptions import NotOpenSSLWarning

        warnings.filterwarnings("ignore", category=NotOpenSSLWarning)
    except Exception:
        pass
    warnings.filterwarnings("ignore", message=r"Module 'speechbrain\\.pretrained' was deprecated.*")
    setup_logging()
    _setup_usage_logger()
    logging.getLogger("httpx").setLevel(logging.WARNING)
    logging.getLogger("snapscript").setLevel(logging.INFO)

    logging.getLogger("local_telegram_bot").info("Python exe=%s prefix=%s", sys.executable, sys.prefix)
    _acquire_bot_lock(os.path.join(os.path.dirname(__file__), ".bot.lock"))

    token = (os.environ.get("TELEGRAM_BOT_TOKEN") or "").strip()
    if not token:
        raise SystemExit("Set TELEGRAM_BOT_TOKEN (or create .env from .env.example).")

    async def post_init(app: Application) -> None:
        try:
            me = await app.bot.get_me()
            if me.username:
                app.bot_data["bot_username"] = me.username
                logging.getLogger("local_telegram_bot").info("Bot username=@%s", me.username)
            else:
                logging.getLogger("local_telegram_bot").warning("Bot has no username; group @mention flow won't work.")
        except Exception:
            logging.getLogger("local_telegram_bot").exception("Failed to fetch bot username via getMe()")

    builder = Application.builder().token(token).post_init(post_init)
    try:
        cu = _bot_concurrent_updates()
        builder = builder.concurrent_updates(cu)
        logging.getLogger("local_telegram_bot").info("Bot concurrent updates: %s", cu)
    except Exception:
        logging.getLogger("local_telegram_bot").exception("Failed to enable concurrent updates; falling back to sequential.")
    app = builder.build()

    async def on_error(update: object, context: ContextTypes.DEFAULT_TYPE) -> None:
        err = context.error
        logging.getLogger("local_telegram_bot").exception("Unhandled error: %r", err)

    app.add_error_handler(on_error)
    app.add_handler(CommandHandler("start", cmd_start))
    app.add_handler(CommandHandler("auth", cmd_auth))
    app.add_handler(CallbackQueryHandler(cb_cancel, pattern=r"^cancel:"))
    app.add_handler(MessageHandler(filters.ChatType.GROUPS & filters.TEXT & filters.REPLY, handle_group_tag))
    app.add_handler(MessageHandler(filters.TEXT & (~filters.COMMAND), handle_process_text))
    app.add_handler(MessageHandler(filters.VOICE | filters.AUDIO | filters.Document.AUDIO, handle_audio))

    app.run_polling(allowed_updates=Update.ALL_TYPES)


if __name__ == "__main__":
    main()
